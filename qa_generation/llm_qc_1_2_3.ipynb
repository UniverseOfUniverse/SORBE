{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8cafafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_content(prompt, images):\n",
    "    image_list = images or [] \n",
    "    content = [\n",
    "        {\"type\": \"image_url\", \"image_url\": img_url}\n",
    "        for img_url in image_list\n",
    "    ] + [\n",
    "        {\"type\": \"text\", \"text\": prompt}\n",
    "    ]\n",
    "    return content\n",
    "\n",
    "def openai_pack_content(prompt, images):\n",
    "    image_list = images or []\n",
    "    content = [\n",
    "        {\"type\": \"image_url\", \"image_url\": {\n",
    "            \"url\": img_url,\n",
    "            \"detail\": \"auto\"\n",
    "        }}\n",
    "        for img_url in image_list\n",
    "    ] + [\n",
    "        {\"type\": \"text\", \"text\": prompt}\n",
    "    ]\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767efc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import asyncio\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from openai import AsyncOpenAI, APIConnectionError, InternalServerError\n",
    "from asyncio import as_completed\n",
    "from tqdm import tqdm\n",
    "import httpx\n",
    "\n",
    "import logging\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "dashscope_api_key = os.getenv(\"DASHSCOPE_API_KEY\") or \"1\"\n",
    "vl_model = \"qwen3-vl-plus\"\n",
    "text_model = \"qwen-plus\"\n",
    "dashscope_client = AsyncOpenAI(\n",
    "    api_key=dashscope_api_key,\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",timeout=120.0)\n",
    "\n",
    "\n",
    "local_text_api_key = \"xxx\"\n",
    "local_text_model = \"xxx\"\n",
    "\n",
    "local_text_client = AsyncOpenAI(\n",
    "    api_key=local_text_api_key,\n",
    "    base_url=\"xxx\", \n",
    "    timeout=120.0\n",
    ")\n",
    "\n",
    "local_vl_api_key = \"xxx\"\n",
    "local_vl_model = \"xxx\"\n",
    "\n",
    "local_vl_client = AsyncOpenAI(\n",
    "    api_key=local_vl_api_key,\n",
    "    base_url=\"xxx\", \n",
    "    timeout=120.0\n",
    ")\n",
    "\n",
    "async def get_response_async(prev_messages,\n",
    "                             next_content,\n",
    "                             model,\n",
    "                             client,\n",
    "                             tools=None,\n",
    "                             max_retries=3):\n",
    "\n",
    "    if isinstance(next_content, str):\n",
    "        user_content = next_content\n",
    "    else:\n",
    "        user_content = next_content\n",
    "\n",
    "    messages = prev_messages + [{\"role\": \"user\", \"content\": user_content}]\n",
    "\n",
    "    MAX_TOKENS_LIMIT = 4096 \n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            reasoning_content = \"\"\n",
    "            answer_content = \"\"\n",
    "            tool_info = []\n",
    "            is_answering = False\n",
    "\n",
    "            if tools is not None:\n",
    "                response = await client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    tools=tools,\n",
    "                    parallel_tool_calls=True,\n",
    "                    stream=True,\n",
    "                    max_tokens=MAX_TOKENS_LIMIT\n",
    "                )\n",
    "            else:\n",
    "                response = await client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    stream=True,\n",
    "                    max_tokens=MAX_TOKENS_LIMIT\n",
    "                )\n",
    "\n",
    "            async for chunk in response:\n",
    "                if chunk.choices:\n",
    "                    delta = chunk.choices[0].delta\n",
    "                    if hasattr(delta, 'reasoning_content'\n",
    "                               ) and delta.reasoning_content != None:\n",
    "                        reasoning_content += delta.reasoning_content\n",
    "                    else:\n",
    "                        if not is_answering:\n",
    "                            is_answering = True\n",
    "                        if delta.content is not None:\n",
    "                            answer_content += delta.content\n",
    "                        if delta.tool_calls is not None:\n",
    "                            for tool_call in delta.tool_calls:\n",
    "                                index = tool_call.index\n",
    "                                while len(tool_info) <= index:\n",
    "                                    tool_info.append({})\n",
    "                                if tool_call.id:\n",
    "                                    tool_info[\n",
    "                                        index]['id'] = tool_info[index].get(\n",
    "                                            'id', '') + tool_call.id\n",
    "                                if tool_call.function and tool_call.function.name:\n",
    "                                    tool_info[index][\n",
    "                                        'name'] = tool_info[index].get(\n",
    "                                            'name',\n",
    "                                            '') + tool_call.function.name\n",
    "                                if tool_call.function and tool_call.function.arguments:\n",
    "                                    tool_info[index][\n",
    "                                        'arguments'] = tool_info[index].get(\n",
    "                                            'arguments',\n",
    "                                            '') + tool_call.function.arguments\n",
    "                                if tool_call.type:\n",
    "                                    tool_info[index]['type'] = tool_call.type\n",
    "\n",
    "            if not reasoning_content:\n",
    "                if answer_content.startswith(\"<think>\"):\n",
    "                    end_think_idx = answer_content.find(\"</think>\")\n",
    "                    if end_think_idx != -1:\n",
    "                        reasoning_content = answer_content[len(\"<think>\"\n",
    "                                                               ):end_think_idx]\n",
    "                        answer_content = answer_content[end_think_idx +\n",
    "                                                        len(\"</think>\"):]\n",
    "\n",
    "            new_message = {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": answer_content,\n",
    "            }\n",
    "            if len(tool_info) > 0:\n",
    "                tool_calls = [{\n",
    "                    \"id\": tool_call[\"id\"],\n",
    "                    \"function\": {\n",
    "                        \"name\": tool_call[\"name\"],\n",
    "                        \"arguments\": tool_call[\"arguments\"]\n",
    "                    },\n",
    "                    \"type\": tool_call[\"type\"],\n",
    "                    \"index\": i\n",
    "                } for i, tool_call in enumerate(tool_info)]\n",
    "                new_message[\"tool_calls\"] = tool_calls\n",
    "            messages.append(new_message)\n",
    "\n",
    "            return {\n",
    "                \"content\": answer_content,\n",
    "                \"reasoning_content\": reasoning_content,\n",
    "                \"usage\": None,\n",
    "                \"prev_messages\": messages,\n",
    "                \"tool_info\": tool_info\n",
    "            }\n",
    "\n",
    "        except (APIConnectionError, InternalServerError) as e:\n",
    "            print(\n",
    "                f\"--- [Retryable Error] (Attempt {attempt + 1}/{max_retries}): {e}\"\n",
    "            )\n",
    "            if attempt == max_retries - 1: raise e\n",
    "            await asyncio.sleep(5)\n",
    "\n",
    "        except Exception as e:\n",
    "            error_str = str(e).lower()\n",
    "            if \"incomplete chunked read\" in error_str or \"peer closed connection\" in error_str or \"connection closed\" in error_str:\n",
    "                print(\n",
    "                    f\"--- [Network/Server Cutoff] (Attempt {attempt + 1}/{max_retries}): {e}\"\n",
    "                )\n",
    "                if attempt == max_retries - 1:\n",
    "                    print(\"--- Max retries reached for cutoff error.\")\n",
    "                    raise e\n",
    "                print(\n",
    "                    \"--- Server likely overloaded. Sleeping for 10 seconds...\")\n",
    "                await asyncio.sleep(10)\n",
    "            else:\n",
    "                print(f\"--- [Fatal Error]: {e}\")\n",
    "                raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e88ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_qa_output(output_str):\n",
    "    output_str = output_str.strip()\n",
    "    if output_str.startswith(\"```json\") and output_str.endswith(\"```\"):\n",
    "        output_str = output_str[len(\"```json\"): -len(\"```\")].strip()\n",
    "    try:\n",
    "        qa_list = json.loads(output_str)\n",
    "        if isinstance(qa_list, list):\n",
    "            return qa_list\n",
    "        else:\n",
    "            print(\"Output is not a list.\")\n",
    "            return []\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decoding error: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703e7195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_quality_score(quality_check_output_str):\n",
    "    score = None\n",
    "    explanation = None\n",
    "    \n",
    "    score_start = quality_check_output_str.index(\"<scores>\") + len(\"<scores>\")\n",
    "    score_end = quality_check_output_str.index(\"</scores>\")\n",
    "    score_json_str = quality_check_output_str[score_start:score_end].strip()\n",
    "    score = json.loads(score_json_str)\n",
    "\n",
    "    explanation_start = quality_check_output_str.index(\"<explanation>\") + len(\"<explanation>\")\n",
    "    explanation_end = quality_check_output_str.index(\"</explanation>\")\n",
    "    explanation = quality_check_output_str[explanation_start:explanation_end].strip()\n",
    "\n",
    "    return score, explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64a77bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer_format_rule(answer_string: str):\n",
    "    required_tags = {\n",
    "        \"description\": [\"<description>\", \"</description>\"],\n",
    "        \"reason\": [\"<reason>\", \"</reason>\"],\n",
    "        \"boxed\": [\"\\\\boxed{\", \"}\"]\n",
    "    }\n",
    "    extracted_contents = {}\n",
    "    for tag, (start_tag, end_tag) in required_tags.items():\n",
    "        start_index = answer_string.find(start_tag)\n",
    "        end_index = answer_string.find(end_tag)\n",
    "        if start_index == -1 or end_index == -1 or start_index >= end_index:\n",
    "            return False, None\n",
    "        extracted_contents[tag] = answer_string[start_index + len(start_tag): end_index].strip()\n",
    "    return True, extracted_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74069642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alter_tags(answer_string: str):\n",
    "    answer_string = answer_string.replace(\"<description>\", \"\\\\<description\\\\>\")\n",
    "    answer_string = answer_string.replace(\"</description>\", \"\\\\</description\\\\>\")\n",
    "    answer_string = answer_string.replace(\"<reason>\", \"\\\\<reason\\\\>\")\n",
    "    answer_string = answer_string.replace(\"</reason>\", \"\\\\</reason\\\\>\")\n",
    "    return answer_string\n",
    "\n",
    "def format_data_md(results):\n",
    "    md_lines = []\n",
    "    for idx, res in enumerate(results):\n",
    "        if res[\"qa_pair\"] is None:\n",
    "            continue\n",
    "        md_lines.append(f\"## Sample {idx + 1}\\n\")\n",
    "        md_lines.append(f\"**Original Sample Index:** {res['index']}\\n\")\n",
    "        md_lines.append(\"**Context:**\\n\")\n",
    "        md_lines.append(f\"{res['context']}\\n\")\n",
    "        md_lines.append(\"**Image Captions:**\\n\")\n",
    "        for idx, caption in enumerate(res[\"image_captions\"], 1):\n",
    "            md_lines.append(f\"- Image {idx}: {caption}\\n\")\n",
    "\n",
    "        keyword_result = res.get('keyword_category_result', 'N/A')\n",
    "        md_lines.append(\"**I. Thematic Classification and Keywords:**\\n\")\n",
    "        md_lines.append(f\"> {keyword_result}\\n\") \n",
    "        md_lines.append(\"---\\n\") \n",
    "\n",
    "        md_lines.append(\"**Question-Answer Pair:**\\n\")\n",
    "        image_indices = res['qa_pair'].get('image_indices', [])\n",
    "        md_lines.append(f\"**Image Indices Used (1-indexed):** {image_indices}\\n\")\n",
    "        md_lines.append(f\"**Question:**\\n{res['qa_pair']['question']}\\n\")\n",
    "        md_lines.append(f\"**Answer:**\\n{alter_tags(res['qa_pair']['answer'])}\\n\")\n",
    "        md_lines.append(\"**Format Check Result:**\\n\")\n",
    "        md_lines.append(f\"{res['format_check']}\\n\")\n",
    "        md_lines.append(\"**Quality Scores:**\\n\")\n",
    "        md_lines.append(f\"```json\\n{json.dumps(res['quality_score'], indent=2)}\\n```\\n\")\n",
    "        md_lines.append(\"**Quality Explanation:**\\n\")\n",
    "        md_lines.append(f\"{alter_tags(res['quality_explanation'])}\\n\")\n",
    "        md_lines.append(\"---\\n\")\n",
    "    return \"\\n\".join(md_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c42cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def build_caption_with_id(img_info):\n",
    "    caption = img_info.get(\"caption\", \"\")\n",
    "    fig_id = img_info.get(\"fig_id\", \"\")\n",
    "    sub_label = img_info.get(\"subfig_label\", \"\")\n",
    "\n",
    "    if fig_id:\n",
    "\n",
    "        prefix = f\"This is {fig_id}{sub_label}. \"\n",
    "        return prefix + caption\n",
    "    else:\n",
    "        return caption\n",
    "\n",
    "def extract_specific_context(item, target_indices):\n",
    "    captions_list = item.get(\"context_enhanced_captions\", [])\n",
    "    summary_data = item.get(\"context_enhanced_summary\", {})\n",
    "    \n",
    "    obs_parts = []\n",
    "    int_parts = []\n",
    "    \n",
    "    for entry in captions_list:\n",
    "        idx = entry.get(\"image_index\")\n",
    "        if idx in target_indices:\n",
    "            obs = entry.get(\"observation\", \"\")\n",
    "            if obs: obs_parts.append(f\"[Image {idx}]: {obs}\")\n",
    "            interp = entry.get(\"interpretation\", \"\")\n",
    "            if interp: int_parts.append(f\"[Image {idx}]: {interp}\")\n",
    "    \n",
    "    if summary_data.get(\"observation_summary\"):\n",
    "        obs_parts.append(f\"[observation_summary]: {summary_data['observation_summary']}\")\n",
    "    if summary_data.get(\"interpretation_summary\"):\n",
    "        int_parts.append(f\"[interpretation_summary]: {summary_data['interpretation_summary']}\")\n",
    "        \n",
    "    return \"\\n\".join(obs_parts), \"\\n\".join(int_parts)\n",
    "\n",
    "\n",
    "def extract_interpretation_text(item, target_indices=None):\n",
    "    captions_list = item.get(\"context_enhanced_captions\", [])\n",
    "    summary_data = item.get(\"context_enhanced_summary\", {})\n",
    "    int_summary = summary_data.get(\"interpretation_summary\", \"\")\n",
    "    combined_int_parts = []\n",
    "    for entry in captions_list:\n",
    "        idx = entry.get(\"image_index\")\n",
    "        if target_indices and idx not in target_indices:\n",
    "            continue\n",
    "        int_text = entry.get(\"interpretation\", \"\")\n",
    "        if int_text and int_text != \"Not found\":\n",
    "            combined_int_parts.append(f\"[Image {idx} Interpretation]: {int_text}\")\n",
    "    if int_summary:\n",
    "        combined_int_parts.append(f\"[Overall Summary]: {int_summary}\")\n",
    "    return \"\\n\".join(combined_int_parts)\n",
    "\n",
    "\n",
    "def format_background_intro(theme_data, target_indices):\n",
    "    if not theme_data:\n",
    "        return \"\"\n",
    "    \n",
    "\n",
    "    exp_bg =theme_data.get(\"Experimental background\", \"N/A\")\n",
    "    \n",
    "\n",
    "    all_themes = theme_data.get(\"Image Settings\", {})\n",
    "    \n",
    "    selected_themes = {}\n",
    "    indices_to_check = target_indices if target_indices else [int(k.replace(\"Image \", \"\")) for k in all_themes.keys() if \"Image\" in k]\n",
    "    \n",
    "    for idx in indices_to_check:\n",
    "        key = f\"Image {idx}\"\n",
    "        if key in all_themes:\n",
    "            selected_themes[key] = all_themes[key]\n",
    "            \n",
    "    background_dict = {\n",
    "        \"Experimental background\": exp_bg,\n",
    "        \"Image Settings\": selected_themes\n",
    "    }\n",
    "    \n",
    "    return json.dumps(background_dict, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3d934b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data_final_v2.json...\n",
      "Done! Saved to data_final_v2_processed.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def generate_flattened_chain(item):\n",
    "    \"\"\"\n",
    "    Convert nested logic_chain into a flattened list of strings and handle index alignment.\n",
    "    \"\"\"\n",
    "    logic_chain = item.get(\"input_logic_chain\", [])\n",
    "    \n",
    "    # Get the main data body\n",
    "    data = logic_chain[0] if isinstance(logic_chain, list) and len(logic_chain) > 0 else logic_chain\n",
    "    \n",
    "    if not isinstance(data, dict):\n",
    "        return []\n",
    "\n",
    "    lines = []\n",
    "\n",
    "    # 1. Research Context\n",
    "    context = data.get(\"research_context\", \"\")\n",
    "    if context:\n",
    "        lines.append(f\"Research Context: {context}\")\n",
    "\n",
    "    # 2. Experiments (Facts)\n",
    "    experiments = data.get(\"experiments\", [])\n",
    "    for i, exp in enumerate(experiments):\n",
    "        setting = exp.get(\"experimental_setting\", \"standard setting\")\n",
    "        phenomenon = exp.get(\"visual_phenomenon\", \"observed phenomenon\")\n",
    "        result = exp.get(\"sub_conclusion\", \"observed result\")\n",
    "        \n",
    "        line = (f\"Experiment {i+1}: In the setting of {setting}, \"\n",
    "                f\"the visual phenomenon observed was {phenomenon}, \"\n",
    "                f\"which indicates {result}.\")\n",
    "        lines.append(line)\n",
    "\n",
    "    # 3. Intermediate Inferences (Logic)\n",
    "    reasoning = data.get(\"reasoning\", {})\n",
    "    inferences = reasoning.get(\"intermediate_inferences\", [])\n",
    "\n",
    "    # Step A: Collect all reference numbers to determine indexing style\n",
    "    all_refs_ints = set()\n",
    "    for inf in inferences:\n",
    "        raw_refs = inf.get(\"based_on_experiments\", [])\n",
    "        for r in raw_refs:\n",
    "            if str(r).isdigit(): \n",
    "                all_refs_ints.add(int(r))\n",
    "    \n",
    "    # Step B: Determine offset (if 0-indexed, add 1)\n",
    "    offset = 1 if (0 in all_refs_ints) else 0\n",
    "\n",
    "    for i, inf in enumerate(inferences):\n",
    "        inference_text = inf.get(\"sub_conclusion\", \"\")\n",
    "        raw_refs = inf.get(\"based_on_experiments\", [])\n",
    "        \n",
    "        corrected_refs = []\n",
    "        for ref in raw_refs:\n",
    "            try:\n",
    "                # Key Fix: Convert to int, apply offset, then back to string\n",
    "                val = int(ref)\n",
    "                corrected_refs.append(str(val + offset))\n",
    "            except (ValueError, TypeError):\n",
    "                # Keep original value if not a digit\n",
    "                corrected_refs.append(str(ref))\n",
    "        \n",
    "        ref_str = \", \".join(corrected_refs)\n",
    "        \n",
    "        line = (f\"Intermediate Inference {i+1}: Derived from Experiment {ref_str}, \"\n",
    "                f\"it is inferred that {inference_text}.\")\n",
    "        lines.append(line)\n",
    "\n",
    "    # 4. Final Conclusion\n",
    "    content = reasoning.get(\"content\", \"\")\n",
    "    conclusion = reasoning.get(\"conclusion\", \"\")\n",
    "    \n",
    "    if content or conclusion:\n",
    "        line = (f\"Final Conclusion: Synthesizing the above, {content}. \"\n",
    "                f\"Therefore, {conclusion}.\")\n",
    "        lines.append(line)\n",
    "\n",
    "    return lines\n",
    "\n",
    "def process_file(file_path):\n",
    "    print(f\"Processing {file_path}...\")\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for item in data:\n",
    "        flattened_list = generate_flattened_chain(item)\n",
    "        item['flattened_logic_chain'] = flattened_list\n",
    "\n",
    "    output_path = file_path.replace(\".json\", \"_processed.json\")\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Done! Saved to {output_path}\")\n",
    "\n",
    "# --- Execution ---\n",
    "input_qa_filename = \"data_final.json\" \n",
    "input_qa_file_prefix = os.path.splitext(input_qa_filename)[0]\n",
    "\n",
    "if os.path.exists(input_qa_filename):\n",
    "    process_file(input_qa_filename)\n",
    "else:\n",
    "    print(f\"File '{input_qa_filename}' not found. Please check the path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "837e56c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGIC_CHAIN_QC_1_TEMPLATE = \\\n",
    "\"\"\"\n",
    "You are an expert in biomedical reasoning and logic evaluation.\n",
    "Your task is to evaluate the integrity and coherence of a logic chain.\n",
    "The input is a structured list of strings representing the progression from experimental facts to intermediate inferences, and finally to a conclusion.\n",
    "\n",
    "# Input Data\n",
    "\n",
    "[Logic Chain] (The reasoning path to evaluate):\n",
    "{flattened_logic_chain}\n",
    "\n",
    "# Evaluation Criteria (1-5 Scale)\n",
    "\n",
    "1. Evidence Support Strength\n",
    "   Assess if the intermediate inferences provide sufficient and accurate support for the final reasoning content.\n",
    "   - Score 1 (Critical Fail): Contradictory or Unsupported. The final content makes claims that contradict the intermediate inferences or relies on evidence not present in the chain.\n",
    "   - Score 3 (Borderline): Weak or Partial Support. The final content is somewhat related but contains major leaps in logic or includes details not fully backed by the intermediate steps.\n",
    "   - Score 5 (Pass): Strong Support. The final content is a robust and accurate synthesis strictly derived from the provided intermediate inferences.\n",
    "\n",
    "2. Logical Flow and Coherence\n",
    "   Assess if the transition from Intermediate Inferences to the Final Conclusion is logically sound and seamless.\n",
    "   - Score 1 (Critical Fail): Fragmented or Disjointed. The logic jumps randomly; the connection between the inference layer and the conclusion layer is broken or nonsensical.\n",
    "   - Score 3 (Borderline): Rough or Repetitive. The flow is understandable but clunky, redundant, or requires the reader to guess the connection between steps.\n",
    "   - Score 5 (Pass): Seamless and Coherent. The reasoning flows naturally like a scientific argument; the conclusion feels like the inevitable result of the preceding steps.\n",
    "\n",
    "# Output Format (Strict JSON)\n",
    "\n",
    "You must return the result strictly in the following format:\n",
    "\n",
    "<scores>\n",
    "{{\n",
    "  \"Evidence Support Strength\": A,\n",
    "  \"Logical Flow and Coherence\": B\n",
    "}}\n",
    "</scores>\n",
    "\n",
    "<explanation>\n",
    "[Provide a brief explanation for your scoring. explicitly stating if there are logical gaps, contradictions, or if the chain is solid.]\n",
    "</explanation>\n",
    "\n",
    "(Where A, B are integer scores from 1 to 5)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4153b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing Logic Chain QC for: data_final_v2_processed.json\n",
      "[INFO] Starting QC for 10 items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:43<00:00,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Writing results...\n",
      "------------------------------\n",
      "[DONE] Finished Logic Chain QC.\n",
      "Passed Items: 10\n",
      "Failed Items: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "INPUT_FILE = f\"{input_qa_file_prefix}_processed.json\"  \n",
    "\n",
    "file_prefix = os.path.splitext(INPUT_FILE)[0]\n",
    "OUTPUT_PASS_FILE = f\"{file_prefix}_qc1_passed.json\" \n",
    "OUTPUT_FAIL_FILE = f\"{file_prefix}_qc1_failed.json\"\n",
    "\n",
    "CONCURRENT_LIMIT = 5\n",
    "\n",
    "async def process_single_logic_chain(item, sem):\n",
    "    \"\"\"\n",
    "    Perform QC specifically for the flattened_logic_chain.\n",
    "    \"\"\"\n",
    "    async with sem:\n",
    "        # 1. Extract logic chain\n",
    "        raw_chain = item.get(\"flattened_logic_chain\", [])\n",
    "        \n",
    "        # Format handling\n",
    "        if isinstance(raw_chain, list):\n",
    "            chain_str = \"\\n\".join(raw_chain)\n",
    "        elif isinstance(raw_chain, str):\n",
    "            chain_str = raw_chain\n",
    "        else:\n",
    "            return False, item, \"Missing or invalid flattened_logic_chain\"\n",
    "\n",
    "        if not chain_str.strip():\n",
    "            return False, item, \"Empty logic chain\"\n",
    "\n",
    "        # 2. Prepare Prompt\n",
    "        prompt = LOGIC_CHAIN_QC_1_TEMPLATE.format(\n",
    "            flattened_logic_chain=chain_str\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            content = openai_pack_content(prompt, None)\n",
    "            response = await get_response_async([], content, local_text_model, local_text_client)\n",
    "            \n",
    "            scores, explanation = extract_quality_score(response[\"content\"])\n",
    "            \n",
    "            if not scores:\n",
    "                return False, item, \"Score parsing failed\"\n",
    "\n",
    "            score_values = [v for k,v in scores.items() if isinstance(v, (int, float))]\n",
    "            if not score_values:\n",
    "                return False, item, \"No valid scores\"\n",
    "            \n",
    "            min_score = min(score_values)\n",
    "            \n",
    "            item[\"logic_qc_scores\"] = scores\n",
    "            item[\"logic_qc_explanation\"] = explanation\n",
    "            \n",
    "            # Threshold setting (e.g., must be > 3)\n",
    "            if min_score > 3: \n",
    "                return True, item, None\n",
    "            else:\n",
    "                return False, item, f\"Low Score (Min: {min_score})\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return False, item, f\"Error: {str(e)}\"\n",
    "\n",
    "async def run_qc_pipeline_single_file():\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"[Error] Input file not found: {INPUT_FILE}\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] Processing Logic Chain QC for: {INPUT_FILE}\")\n",
    "\n",
    "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        data_list = json.load(f)\n",
    "\n",
    "    semaphore = asyncio.Semaphore(CONCURRENT_LIMIT)\n",
    "    \n",
    "    async def process_item_wrapper(item):\n",
    "        # Validate required fields\n",
    "        if \"flattened_logic_chain\" not in item:\n",
    "            return [], [item]\n",
    "\n",
    "        is_passed, processed_item, reason = await process_single_logic_chain(item, semaphore)\n",
    "\n",
    "        batch_passed = [] \n",
    "        batch_failed = [] \n",
    "        \n",
    "        new_item = processed_item.copy() \n",
    "        \n",
    "        if reason:\n",
    "             new_item[\"qc_fail_reason\"] = reason\n",
    "\n",
    "        if is_passed:\n",
    "            batch_passed.append(new_item)\n",
    "        else:\n",
    "            batch_failed.append(new_item)\n",
    "\n",
    "        return batch_passed, batch_failed\n",
    "\n",
    "    print(f\"[INFO] Starting QC for {len(data_list)} items...\")\n",
    "    tasks = [process_item_wrapper(item) for item in data_list]\n",
    "    results = await tqdm_asyncio.gather(*tasks)\n",
    "\n",
    "    all_passed_items = []\n",
    "    all_failed_items = []\n",
    "\n",
    "    for batch_pass_list, batch_fail_list in results:\n",
    "        if batch_pass_list:\n",
    "            all_passed_items.extend(batch_pass_list)\n",
    "        if batch_fail_list:\n",
    "            all_failed_items.extend(batch_fail_list)\n",
    "\n",
    "    print(f\"[INFO] Writing results...\")\n",
    "    \n",
    "    with open(OUTPUT_PASS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_passed_items, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    with open(OUTPUT_FAIL_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_failed_items, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"-\" * 30)\n",
    "    print(f\"[DONE] Finished Logic Chain QC.\")\n",
    "    print(f\"Passed Items: {len(all_passed_items)}\")\n",
    "    print(f\"Failed Items: {len(all_failed_items)}\")\n",
    "\n",
    "# Execution\n",
    "await run_qc_pipeline_single_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc27b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件: data_final_v2_processed_qc1_passed.json\n",
      "处理完成！已更新 10 条数据。\n",
      "结果已保存回: data_final_v2_processed_qc1_passed.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "def extract_visual_phenomena(file_path):\n",
    "    # Pre-process data before QC2\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File {file_path} not found\")\n",
    "        return\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    count = 0\n",
    "    for item in data:\n",
    "        extracted_list = []\n",
    "        \n",
    "        logic_chain = item.get(\"input_logic_chain\", [])\n",
    "        \n",
    "        if logic_chain and isinstance(logic_chain, list) and len(logic_chain) > 0:\n",
    "            first_chain_node = logic_chain[0]\n",
    "            experiments = first_chain_node.get(\"experiments\", [])\n",
    "            \n",
    "            for exp in experiments:\n",
    "                vp_text = exp.get(\"visual_phenomenon\", \"\")\n",
    "                \n",
    "                if vp_text:\n",
    "                    # 1. Regex: Remove spaces and brackets [...] content\n",
    "                    vp_text = re.sub(r'\\s*\\[.*?\\]', '', vp_text)\n",
    "                    \n",
    "                    # 2. Strip whitespace\n",
    "                    vp_text = vp_text.strip()\n",
    "                    \n",
    "                    # 3. Fix potential \" .\" cases\n",
    "                    vp_text = vp_text.replace(' .', '.')\n",
    "                    \n",
    "                    # 4. Ensure trailing period exists\n",
    "                    if len(vp_text) > 0 and not vp_text.endswith('.'):\n",
    "                        vp_text += '.'\n",
    "\n",
    "                extracted_list.append({\n",
    "                    \"visual_phenomenon\": vp_text\n",
    "                })\n",
    "        \n",
    "        # Write new field to item\n",
    "        item[\"extracted_visual_phenomena\"] = extracted_list\n",
    "        count += 1\n",
    "\n",
    "    # Save back to file\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Done! Updated {count} items.\")\n",
    "    print(f\"Results saved to: {file_path}\")\n",
    "\n",
    "# --- Execution ---\n",
    "input_qa_filename = f\"{input_qa_file_prefix}_processed_qc1_passed.json\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_visual_phenomena(input_qa_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37287cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGIC_CHAIN_QC_2_TEMPLATE = \\\n",
    "\"\"\"\n",
    "You are an expert in biomedical text verification and fact-checking.\n",
    "Your task is to verify if the [Visual Phenomena] described in the logic chain are supported by the provided Source Data ([Observation] and [Context]).\n",
    "\n",
    "# Input Data\n",
    "\n",
    "[Observation] (Objective visual descriptions of the images):\n",
    "{Observation}\n",
    "\n",
    "[Context] (Background containing [Image] tags):\n",
    "{Context}\n",
    "\n",
    "[Visual Phenomena] (The descriptions extracted from the logic chain to be verified):\n",
    "{VisualPhenomena}\n",
    "\n",
    "# Evaluation Criteria (1-5 Scale)\n",
    "\n",
    "1. Source Grounding & Verification\n",
    "   Assess if every visual phenomenon listed in the Target is explicitly mentioned or clearly visible in the [Context] or [Observation].\n",
    "   - Score 1 (Critical Fail): Hallucination. The target describes features that are completely absent from both the Observation and Context, or contradicts them.\n",
    "   - Score 3 (Borderline): Partial Match. Some descriptions are supported, but others are missing source evidence, or the target adds significant details not found in the source.\n",
    "   - Score 5 (Pass): Fully Grounded. Every statement in the [Visual Phenomena] is directly supported by evidence found in the Source Observation or Source Context (textual descriptions of visual outcomes).\n",
    "\n",
    "# Output Format (Strict JSON)\n",
    "\n",
    "You must return the result strictly in the following format:\n",
    "\n",
    "<scores>\n",
    "{{\n",
    "  \"Source Grounding & Verification\": A\n",
    "}}\n",
    "</scores>\n",
    "\n",
    "<explanation>\n",
    "[Provide a brief explanation. If there is a hallucination or missing reference, explicitly quote the unsupported part.]\n",
    "</explanation>\n",
    "\n",
    "(Where A is an integer score from 1 to 5)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fd0dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing QC2 (Grounding Check) for: data_final_v2_processed_qc1_passed.json\n",
      "[INFO] Starting QC2 for 10 items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:30<00:00,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Writing QC2 results...\n",
      "------------------------------\n",
      "[DONE] Finished QC2.\n",
      "Passed Items: 10\n",
      "Failed Items: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "# ================= Configuration =================\n",
    "INPUT_FILE = f\"{input_qa_file_prefix}_processed_qc1_passed.json\"\n",
    "\n",
    "file_prefix = f\"{input_qa_file_prefix}_processed\"\n",
    "OUTPUT_PASS_FILE = f\"{file_prefix}_qc2_passed.json\"\n",
    "OUTPUT_FAIL_FILE = f\"{file_prefix}_qc2_failed.json\"\n",
    "\n",
    "CONCURRENT_LIMIT = 10  # Concurrency limit\n",
    "\n",
    "# ================= Core Processing Functions =================\n",
    "\n",
    "async def process_single_grounding_check(item, sem):\n",
    "    async with sem:\n",
    "        # 1. Extract source data\n",
    "        obs = item.get(\"input_observation\", \"No observation provided.\")\n",
    "        ctx = item.get(\"input_context\", \"No context provided.\")\n",
    "        \n",
    "        # 2. Extract visual_phenomena for verification\n",
    "        extracted_data = item.get(\"extracted_visual_phenomena\", [])\n",
    "        \n",
    "        if not extracted_data:\n",
    "            return False, item, \"No extracted visual phenomena found\"\n",
    "\n",
    "        vp_lines = []\n",
    "        for idx, entry in enumerate(extracted_data):\n",
    "            text = entry.get(\"visual_phenomenon\", \"\")\n",
    "            if text:\n",
    "                vp_lines.append(f\"{idx + 1}. {text}\")\n",
    "        \n",
    "        vp_str = \"\\n\".join(vp_lines)\n",
    "        \n",
    "        if not vp_str.strip():\n",
    "             return False, item, \"Empty visual phenomena text\"\n",
    "\n",
    "        # 3. Fill Prompt\n",
    "        prompt = LOGIC_CHAIN_QC_2_TEMPLATE.format(\n",
    "            Observation=obs,\n",
    "            Context=ctx,\n",
    "            VisualPhenomena=vp_str\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # 4. Call Model\n",
    "            content = openai_pack_content(prompt, None)\n",
    "            response = await get_response_async([], content, local_text_model, local_text_client)\n",
    "            \n",
    "            # 5. Parse scores\n",
    "            scores, explanation = extract_quality_score(response[\"content\"])\n",
    "            \n",
    "            if not scores:\n",
    "                return False, item, \"Score parsing failed\"\n",
    "\n",
    "            # Get target score\n",
    "            score_val = scores.get(\"Source Grounding & Verification\", 0)\n",
    "            \n",
    "            # Write results back to item\n",
    "            item[\"qc2_grounding_score\"] = score_val\n",
    "            item[\"qc2_grounding_explanation\"] = explanation\n",
    "            \n",
    "            # 6. Thresholding (e.g., > 3 passed)\n",
    "            if score_val > 3:\n",
    "                return True, item, None\n",
    "            else:\n",
    "                return False, item, f\"Low Grounding Score ({score_val})\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return False, item, f\"Error: {str(e)}\"\n",
    "\n",
    "async def run_qc2_pipeline():\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"[Error] Input file not found: {INPUT_FILE}\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] Processing QC2 (Grounding Check) for: {INPUT_FILE}\")\n",
    "\n",
    "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        data_list = json.load(f)\n",
    "\n",
    "    semaphore = asyncio.Semaphore(CONCURRENT_LIMIT)\n",
    "    \n",
    "    async def process_item_wrapper(item):\n",
    "        if \"extracted_visual_phenomena\" not in item:\n",
    "             return [], [item] \n",
    "\n",
    "        is_passed, processed_item, reason = await process_single_grounding_check(item, semaphore)\n",
    "\n",
    "        batch_passed = [] \n",
    "        batch_failed = [] \n",
    "        \n",
    "        new_item = processed_item.copy()\n",
    "        \n",
    "        if reason:\n",
    "             new_item[\"qc2_fail_reason\"] = reason\n",
    "\n",
    "        if is_passed:\n",
    "            batch_passed.append(new_item)\n",
    "        else:\n",
    "            batch_failed.append(new_item)\n",
    "\n",
    "        return batch_passed, batch_failed\n",
    "\n",
    "    print(f\"[INFO] Starting QC2 for {len(data_list)} items...\")\n",
    "    tasks = [process_item_wrapper(item) for item in data_list]\n",
    "    results = await tqdm_asyncio.gather(*tasks)\n",
    "\n",
    "    all_passed_items = []\n",
    "    all_failed_items = []\n",
    "\n",
    "    for batch_pass_list, batch_fail_list in results:\n",
    "        if batch_pass_list:\n",
    "            all_passed_items.extend(batch_pass_list)\n",
    "        if batch_fail_list:\n",
    "            all_failed_items.extend(batch_fail_list)\n",
    "\n",
    "    print(f\"[INFO] Writing QC2 results...\")\n",
    "    \n",
    "    with open(OUTPUT_PASS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_passed_items, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    with open(OUTPUT_FAIL_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_failed_items, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"-\" * 30)\n",
    "    print(f\"[DONE] Finished QC2.\")\n",
    "    print(f\"Passed Items: {len(all_passed_items)}\")\n",
    "    print(f\"Failed Items: {len(all_failed_items)}\")\n",
    "\n",
    "# Execute\n",
    "await run_qc2_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31c2fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGIC_CHAIN_QC_3_TEMPLATE = \\\n",
    "\"\"\"\n",
    "You are an expert in evaluating question-answering logic.\n",
    "Your task is to verify if the provided Conclusion effectively answers or corresponds to the specific Question asked.\n",
    "\n",
    "# Input Data\n",
    "\n",
    "Question:\n",
    "{Question}\n",
    "\n",
    "Observation (Visual Evidence containing scale info):\n",
    "{Observation}\n",
    "\n",
    "Logic Chain:\n",
    "{LogicChain}\n",
    "\n",
    "Conclusion (Derived from Logic Chain):\n",
    "{Conclusion}\n",
    "\n",
    "# Evaluation Criteria (1-5 Scale)\n",
    "\n",
    "1. Question-Conclusion Alignment\n",
    "   Assess if the Conclusion directly addresses the core inquiry of the Question.\n",
    "   - Score 1 (Fail): The conclusion is irrelevant, unrelated, or contradicts the premise of the question. It does not provide an answer.\n",
    "   - Score 3 (Passable): The conclusion is related and provides a partial answer, but may be slightly tangential or misses the specific format requested.\n",
    "   - Score 5 (Pass): The conclusion provides a clear, logical, and direct answer to the question. It functions effectively as the final output.\n",
    "\n",
    "2. Scale/Legend Consistency Check\n",
    "Check if the problem statement lacks a scale/legend, but the observation results, reasoning content, and conclusion clearly include scale numbers or scale information.\n",
    "- Score 1 point (Serious Failure): The problem statement lacks a scale/legend, but the observation results contain explicit scale numbers (e.g., \"50 nm,\" \"scale\"), and the reasoning content utilizes this scale information from the observation.\n",
    "- Score 5 points (Pass): The problem statement and observation results are consistent; either both include a scale/legend, or neither includes scale-related information. If the problem statement includes scale-related information, but the conclusion and reasoning content do not use it, it is not considered an error.\n",
    "\n",
    "3. Reasoning Validity \n",
    "   Assess if the Logic Chain steps contains excessive speculation or hallucinations not supported by the Observation.\n",
    "   - Score 1 (Critical Fail): Given ONLY Research Context, Experimental Settings, and Visual Phenomenon, the \"inference\", \"sub_conclusion\", \"content\", \"conclusion\" parts contains details impossible to know.\n",
    "   - Score 5 (Pass): Given ONLY Research Context, Experimental Settings, and Visual Phenomenon, the \"inference\", \"sub_conclusion\", \"content\", \"conclusion\" parts are all supported without any hallucination.\n",
    "\n",
    "# Output Format (Strict JSON)\n",
    "\n",
    "You must return the result strictly in the following format:\n",
    "\n",
    "<scores>\n",
    "{{\n",
    "  \"Question-Conclusion Alignment\": A,\n",
    "  \"Scale/Legend Consistency Check\" : B,\n",
    "  \"Reasoning Validity\" : C\n",
    "}}\n",
    "</scores>\n",
    "\n",
    "<explanation>\n",
    "[Briefly explain why the conclusion satisfies or fails to answer the question.]\n",
    "</explanation>\n",
    "\n",
    "\n",
    "(Where A, B, C is an integer score from 1 to 5)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60bcac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing QC3 (Alignment Check) for: data_final_v2_processed_qc2_passed.json\n",
      "[INFO] Starting QC3 for 10 items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:13<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Writing QC3 results...\n",
      "------------------------------\n",
      "[DONE] Finished QC3.\n",
      "Passed Items: 10\n",
      "Failed Items: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "# ================= Configuration =================\n",
    "INPUT_FILE = f\"{input_qa_file_prefix}_processed_qc2_passed.json\"\n",
    "\n",
    "file_prefix = f\"{input_qa_file_prefix}_processed\"\n",
    "OUTPUT_PASS_FILE = f\"{file_prefix}_qc3_passed.json\"\n",
    "OUTPUT_FAIL_FILE = f\"{file_prefix}_qc3_failed.json\"\n",
    "\n",
    "CONCURRENT_LIMIT = 20  # Concurrency limit\n",
    "\n",
    "\n",
    "# ================= Core Processing Functions =================\n",
    "\n",
    "async def process_single_alignment_check(item, sem):\n",
    "    async with sem:\n",
    "        # 1. Extract Question\n",
    "        # Path: basic_qa -> question\n",
    "        question_text = item.get(\"basic_qa\", {}).get(\"question\", \"\")\n",
    "        if not question_text:\n",
    "             return False, item, \"No question found in basic_qa\"\n",
    "        \n",
    "        observation_text = item.get(\"input_observation\", \"No observation provided.\")\n",
    "        \n",
    "        reasoning_content = \"\"\n",
    "        try:\n",
    "            # Logic for reasoning content extraction\n",
    "            logic_chain = item.get(\"input_logic_chain\", [])\n",
    "            if logic_chain and isinstance(logic_chain, list):\n",
    "                first_node = logic_chain[0]\n",
    "                reasoning_node = first_node.get(\"reasoning\", {})\n",
    "                reasoning_content = reasoning_node.get(\"content\", \"\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        if not reasoning_content:\n",
    "            reasoning_content = \"No detailed reasoning content provided.\"\n",
    "\n",
    "        # 2. Extract Conclusion\n",
    "        # Path: input_logic_chain -> first item -> conclusion\n",
    "        conclusion_text = \"\"\n",
    "        try:\n",
    "            logic_chain = item.get(\"input_logic_chain\", [])\n",
    "            if logic_chain and isinstance(logic_chain, list):\n",
    "                first_node = logic_chain[0]\n",
    "                # Primary attempt: direct conclusion field\n",
    "                conclusion_text = first_node.get(\"conclusion\", \"\")\n",
    "                \n",
    "                # Fallback: check inside reasoning node\n",
    "                if not conclusion_text and \"reasoning\" in first_node:\n",
    "                     conclusion_text = first_node.get(\"reasoning\", {}).get(\"conclusion\", \"\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if not conclusion_text:\n",
    "             return False, item, \"No conclusion found in logic chain\"\n",
    "\n",
    "        # 3. Prepare Prompt\n",
    "        prompt = LOGIC_CHAIN_QC_3_TEMPLATE.format(\n",
    "            Question=question_text,\n",
    "            Observation=observation_text,\n",
    "            LogicChain=logic_chain,\n",
    "            Conclusion=conclusion_text\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # 4. Call Model\n",
    "            content = openai_pack_content(prompt, None)\n",
    "            response = await get_response_async([], content, local_text_model, local_text_client)\n",
    "            \n",
    "            # 5. Parse Scores\n",
    "            scores, explanation = extract_quality_score(response[\"content\"])\n",
    "            \n",
    "            if not scores:\n",
    "                return False, item, \"Score parsing failed\"\n",
    "\n",
    "            # QC3 evaluates multiple alignment dimensions\n",
    "            score_alignment = scores.get(\"Question-Conclusion Alignment\", 0)\n",
    "            score_scale = scores.get(\"Scale/Legend Consistency Check\", 0)\n",
    "            score_reasoning = scores.get(\"Reasoning Validity\", 0)\n",
    "            \n",
    "            item[\"qc3_scores\"] = scores\n",
    "            item[\"qc3_explanation\"] = explanation\n",
    "            \n",
    "            min_score = min(score_alignment, score_scale, score_reasoning)\n",
    "            \n",
    "            if min_score > 3:\n",
    "                return True, item, None\n",
    "            else:\n",
    "                return False, item, f\"Low Score (Min: {min_score}, Details: {scores})\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return False, item, f\"Error: {str(e)}\"\n",
    "\n",
    "async def run_qc3_pipeline():\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"[Error] Input file not found: {INPUT_FILE}\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] Processing QC3 (Alignment Check) for: {INPUT_FILE}\")\n",
    "\n",
    "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        data_list = json.load(f)\n",
    "\n",
    "    semaphore = asyncio.Semaphore(CONCURRENT_LIMIT)\n",
    "    \n",
    "    async def process_item_wrapper(item):\n",
    "        if \"basic_qa\" not in item or \"input_logic_chain\" not in item:\n",
    "             return [], [item] \n",
    "\n",
    "        is_passed, processed_item, reason = await process_single_alignment_check(item, semaphore)\n",
    "\n",
    "        batch_passed = [] \n",
    "        batch_failed = [] \n",
    "        \n",
    "        new_item = processed_item.copy()\n",
    "        \n",
    "        if reason:\n",
    "             new_item[\"qc3_fail_reason\"] = reason\n",
    "\n",
    "        if is_passed:\n",
    "            batch_passed.append(new_item)\n",
    "        else:\n",
    "            batch_failed.append(new_item)\n",
    "\n",
    "        return batch_passed, batch_failed\n",
    "\n",
    "    print(f\"[INFO] Starting QC3 for {len(data_list)} items...\")\n",
    "    tasks = [process_item_wrapper(item) for item in data_list]\n",
    "    results = await tqdm_asyncio.gather(*tasks)\n",
    "\n",
    "    all_passed_items = []\n",
    "    all_failed_items = []\n",
    "\n",
    "    for batch_pass_list, batch_fail_list in results:\n",
    "        if batch_pass_list:\n",
    "            all_passed_items.extend(batch_pass_list)\n",
    "        if batch_fail_list:\n",
    "            all_failed_items.extend(batch_fail_list)\n",
    "\n",
    "    print(f\"[INFO] Writing QC3 results...\")\n",
    "    \n",
    "    with open(OUTPUT_PASS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_passed_items, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    with open(OUTPUT_FAIL_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_failed_items, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"-\" * 30)\n",
    "    print(f\"[DONE] Finished QC3.\")\n",
    "    print(f\"Passed Items: {len(all_passed_items)}\")\n",
    "    print(f\"Failed Items: {len(all_failed_items)}\")\n",
    "\n",
    "# Execution\n",
    "await run_qc3_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_qa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
