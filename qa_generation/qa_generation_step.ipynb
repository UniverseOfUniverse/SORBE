{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ef4614",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BIOMED_CHECK_PROMPT = \"\"\"\n",
    "You are a data classifier. Your task is to determine if the provided text context describes Biomedical, Medical, or Clinical content (e.g., pathology, anatomy, cell biology, medical imaging, clinical reports).\n",
    "\n",
    "Input Context:\n",
    "{context}\n",
    "\n",
    "Output Requirement:\n",
    "Return ONLY a JSON object with a single boolean field `is_biomedical`.\n",
    "Example: {{\"is_biomedical\": true}} or {{\"is_biomedical\": false}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "BACKGROUND_DISTILLATION_PROMPT_TEMPLATE = \"\"\"\n",
    "As an expert biomedically scientific editor, your task is to distill the provided [Background] text into a concise, focusing on biomedical entity information.\n",
    "\n",
    "Input:\n",
    "[Background]:\n",
    "{back_info}\n",
    "\n",
    "The distilled summary MUST meet the following requirements:\n",
    "1.  Length: a reasonable summary between 100 and 200 words.\n",
    "2.  Focus: Focus on explaining the core scientific problems within the background context, including the important knowledge related to them..\n",
    "3.  Style: Use formal, clear, and objective scientific language.\n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "KEYWORD_Category_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a top-tier biomedical research analyst, skilled at structured information extraction and thematic classification. Your task is to perform a two-step analysis on the provided [Context] and [Image_caption].\n",
    "\n",
    "I. INPUT DATA\n",
    "\n",
    "[Context]:\n",
    "{context}\n",
    "*(Note: The [Context] may contain `[Image]` tokens indicating image positions. You CANNOT see these images; you MUST rely *only* on the Observation for all visual details.)*\n",
    "\n",
    "[Image_caption]:\n",
    "{image_caption}\n",
    "\n",
    "II. STEP 1: THEMATIC CLASSIFICATION\n",
    "Analyze the content and select **ONE** Main Category (1-4) that best describes the core research domain. Use the professional framework below.\n",
    "\n",
    "CLASSIFICATION FRAMEWORK:\n",
    "- Basic Medical Science :\n",
    "  Focuses on the fundamental mechanisms of life and disease.\n",
    "  (Keywords: Molecular biology, genetics, biochemistry, immunology, physiology, anatomy, neurosciences, cellular pathways, pathogenesis models).\n",
    "- Clinical Medicine :\n",
    "  Focuses on the diagnosis, treatment, and management of human diseases in patients.\n",
    "  (Keywords: Specific diseases (e.g., Heart Disease, Endocarditis), surgical procedures, patient case studies, treatment outcomes, clinical neurology, ophthalmology, urology, orthopaedics).\n",
    "- Diagnostics & Laboratory Medicine :\n",
    "  Focuses on the methods and technologies used to detect and diagnose diseases.\n",
    "  (Keywords: Pathology, histopathology, cytopathology, medical imaging (Radiology, MRI, CT), biomarkers, lab tests, assay development, neuropathology, forensic analysis, electrocardiorgraphy).\n",
    "- Pharmacy & Therapeutics :\n",
    "  Focuses on the discovery, development, and application of drugs.\n",
    "  (Keywords: Pharmacology, drug synthesis, medicinal chemistry, drug targets, therapeutic strategies, drug resistance, clinical trials for drugs, pharmaceutical sciences).\n",
    "\n",
    "III. STEP 2: THEME-GUIDED KEYWORD EXTRACTION\n",
    "Based on the [Context] and the provided Observation and Interpretation as well as the classification result from STEP 1, extract a list of 10-15 highly specific biological or medical keywords.\n",
    "- **CRITICAL:** Ensure keywords are directly relevant to the selected Main Category.\n",
    "- **Focus on:** Specific protein/gene names, cell types/morphologies, disease names, diagnostic criteria (e.g., grading), specific drugs, or key experimental findings.\n",
    "- **Avoid:** Generic words or phrases.\n",
    "\n",
    "\n",
    "\n",
    "IV. REQUIRED OUTPUT FORMAT\n",
    "Output *only* the result in the following exact structure:\n",
    "[Main Category Name]: keyword1, keyword2, keyword3, ..., keyword15\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "VLM_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert biologist and biomedical researcher. You will be given a image and [Image Captions].Your task is to describe the visual content of the image. \n",
    "\n",
    "# Input Data\n",
    "[Image Caption]: \n",
    "{caption}\n",
    "\n",
    "# Task\n",
    "Provide A detailed description of the visual features present in the image,  grounded in the [Image Captions], Avoid using any conclusive statements. Focusing on the observation of visual features in biomedical images.\n",
    "\n",
    "# Constraints\n",
    "- Do NOT output a list. \n",
    "- Do NOT mention \"Image 1\" or other image indices.\n",
    "- Output ONLY the description paragraph.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "CONSENSUS_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a senior biomedical image analyst. You are a senior biomedical image analyst. You will receive observations from four different biomedical experts regarding the same biomedical image.These observations may include their interpretations or inferences based on the image, which you should disregard.\n",
    "\n",
    "#Task:\n",
    "You should limit yourself to purely visual descriptions, avoid adding any explanatory logic, and extract the common visual information from these observations, while avoiding contradictions and ensuring the information is biomedically right. Generate a highly accurate \"comprehensive observation report.\n",
    "\n",
    "## Bad example:It combines Qwenvl's incorrect \"cytoplasmic\" description with a correct \"prominent nuclear staining\" description later,  resulting in a confusing and anatomically impossible description for a single stain.\n",
    "\n",
    "# Input Observations:\n",
    "\n",
    "[Model: Fleming]:\n",
    "{desc_fleming}\n",
    "\n",
    "[Model: Hulu]:\n",
    "{desc_hulu}\n",
    "\n",
    "[Model: Lingshu]:\n",
    "{desc_lingshu}\n",
    "\n",
    "[Model: QwenVL]:\n",
    "{desc_qwenvl}\n",
    "\n",
    "# Key Requirements:\n",
    "1. Voting & Merging Strategy: \n",
    "    - For overlapping features mentioned by multiple models, use **majority voting** to establish the **corroborated facts**.\n",
    "    - For distinct/unique details mentioned by only one model, **naturally merge** them into the description to enrich detail, provided they DO NOT contradict the **corroborated facts** or biomedical logic.\n",
    "\n",
    "2. Pure Observation: Describe ONLY the visible morphological features ( e.g. cells, staining, structures   ). Do not include any reasoning, such as \"consistent with...\", \"indicates...\", \"seems to...\", \"suggests some expression...\", \"may represent...\".  Focus on the visual features of the image itself; do not draw conclusions or inferences based on interpretations or deductions from the image.\n",
    "3. Integration: Output a single, coherent paragraph merging the corroborated facts and valid unique details naturally.\n",
    "\n",
    "# Output\n",
    "(Output ONLY the Integration description paragraph.)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "ENHANCED_CAPTION_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert biologist and biomedical researcher. You will be given [Context], [Background], [Keywords], and a set of initial [Image_captions].\n",
    "\n",
    "# Your goal is to generate a \"Context_Enhanced_Captions\" object. You must process the data in a distinct step for each image: Verification ([Observation]).\n",
    "\n",
    "## Verification ([Observation]): Rigorously validate the [Image_captions] to correct only factual errors based on [Context] while strictly preserving all non-conflicting visual details, ensuring the output remains a purely descriptive report devoid of any explanatory logic.\n",
    "\n",
    "# Input Data\n",
    "\n",
    "[Background]:\n",
    "{distilled_background}\n",
    "\n",
    "[Keywords]:\n",
    "{keywords}\n",
    "\n",
    "[Context]:\n",
    "{context}\n",
    "\n",
    "[Image_captions]:\n",
    "{vl_captions_json}\n",
    "\n",
    "# Task Guidelines & Logic\n",
    "\n",
    "# 1. Alignment Strategy\n",
    "The [Context] text contains `[Image]` tokens (e.g., [Image 1], [Image 2]). These tokens mark the exact location where the image is discussed.\n",
    "You must use the text immediately surrounding these `[Image]` tokens to verify the identity and features of the corresponding image in [Image_captions].\n",
    "\n",
    "# 2. Field: \"observations\" (Strict Visual Verification)\n",
    "Goal: Correct the [Image_captions] ONLY if they are factually wrong based on the [Context], while preserving correct visual details.\n",
    "## Minimal Modification Rule: Do not rewrite the caption if it is consistent with the text. Only edit specific words or phrases that contradict the [Context].\n",
    "### Correction Protocol:\n",
    "    * If [Image_captions] says \"blue stain\" but [Context] specifies \"red stain\", change it to \"red stain\".\n",
    "    * If [Image_captions] mentions a visual detail (e.g., \"irregular shape\") that is NOT mentioned in [Context], PRESERVE IT. Do not delete valid visual details just because the text doesn't mention them.\n",
    "### Anti-Hallucination Rule**: Do NOT add biological reasoning, causal relationships, or background knowledge into this field. Keep it purely descriptive (shapes, colors, positions ).\n",
    "\n",
    "# 3. Summary Generation\n",
    "## [Observation] summary: If there is only one image, provide a concise visual overview of that specific image. If there are multiple images, synthesize the common visual themes across all panels. MUST remain purely descriptive (no reasoning).\n",
    "\n",
    "# Output Format\n",
    "Provide the final answer as a JSON object with a single root key \"Context_Enhanced_Captions\".\n",
    "The output must strictly separate visual descriptions from analytical insights.\n",
    "Ensure the output is a valid JSON list of strings within the structure, corresponding one-to-one with the original captions.\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"Context_Enhanced_Captions\": {{\n",
    "    \"observations\": {{\n",
    "      \"Image 1\": \"...\",\n",
    "      \"Image 2\": \"...\",\n",
    "      ...\n",
    "      \"summary\": \"...\"\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "\n",
    "# Key Requirements\n",
    "1. JSON Validity: The output must be directly parseable by json.loads.\n",
    "2. Context Fidelity: Do not hallucinate details not present in the image or the text.\n",
    "3. Count Match: The number of keys in the dictionary must match the number of input images.\n",
    "\n",
    "Generate [Context-Enhanced Captions]: \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "VISUAL_ELEMENT_QA_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert in biomedical image analysis. Your task is to generate a vision-centered [Question]-[Answer] pairs based ONLY on the provided [Observation].Extract a list of all unique biomedical entities (e.g., cell types, staining, anatomical structures) mentioned in the [Observation].\n",
    "\n",
    "# Input Data\n",
    "[Observation] refers to objective visual descriptions of each images, and the summary consolidates the visual findings to provide a holistic overview,inter image relationship across the images\n",
    "{Observation}\n",
    "\n",
    "# Output\n",
    "\n",
    "Generate a [Question]-[Answer] pair that asks for the specific biomedical visual features mentioned in the [Observation]. If there is only one image available, then only use that single image for the question.If there are multiple images, selecting several (but not necessarily all) closely related images can generate reasonable questions.\n",
    "The goal is to verify if the model can \"see\" the low-level details before performing high-level reasoning.\n",
    "\n",
    "## Key Requirements (MUST FOLLOW):\n",
    "1. Strictly Visual: The [Question] MUST focus ONLY on visual attributes. Aim for high diversity in question suitable for multi-image biomedical analysis. \n",
    "IF [Observation] contains Only One Image**: You MUST generate a Descriptive question specific to that image (e.g., \"Describe the staining intensity of the cytoplasm in Image 1.\"). Do NOT hallucinate other images.\n",
    "\n",
    "IF [Observation] contains multiple images, you can generate questions about the relationships between these images.\n",
    "\n",
    "Examples include:\n",
    "Comparative Morphology: 'Compare the nuclear irregularity observed in different images.'\n",
    "Feature Characterization: 'Describe the texture and staining intensity of the cytoplasm in...'\n",
    "Structural Architecture: 'How does the arrangement of inflammatory cells differ between different images?'\n",
    "\n",
    "2. No Interpretation: The [Question] and [Answer] MUST NOT contain diagnostic conclusions, biological significance, or \"Why\" reasoning. Do not use words like \"suggests\", \"indicates\", or \"diagnosis\".\n",
    "3. Image Reference in [Question]:\n",
    "    - **IF [Observation] contains Multiple Images**:  The [Question] string MUST include at least 2 explicit image references (e.g., [Image 1, Image2, Image3, ...]).\n",
    "    - **IF [Observation] contains Only One Image**: You MUST generate a  question specific to that image. Do NOT hallucinate other images.The [Question] string MUST include  explicit image references (e.g., Image 1).\n",
    "4. Fact-Based: The [Answer] must be rely on the [Observation] text.\n",
    "5. Atomic [Question]: the [Question] string must be a single query. It MUST NOT be a compound question or contain any sub-questions.\n",
    "\n",
    "## OUTPUT FORMAT AND CONSTRAINTS (MUST FOLLOW):\n",
    "Return a valid JSON **List** of objects:\n",
    "```json\n",
    "[\n",
    "  {{\n",
    "    \"qa_pairs\" : {{\n",
    "      \"qa1\": {{\n",
    "        \"question\": \"...\",\n",
    "        \"answer\": \"...\",\n",
    "      }},\n",
    "      \"qa2\": {{\n",
    "        \"question\": \"...\",\n",
    "        \"answer\": \"...\",\n",
    "      }},\n",
    "      ...\n",
    "    }}\n",
    "    \"image_indices\": [...],\n",
    "    \"biomedical_entities\": [\"entity1\", \"entity2\", \"...\"]\n",
    "  }}\n",
    "]\n",
    "\n",
    "Task\n",
    "Generate exactly mltiple visual description QA pair based on the [Observation]. \n",
    "2. Extract a list of all unique biomedical entities (e.g., cell types, staining, anatomical structures) mentioned in the [Observation] and put them in \"biomedical_entities\".\n",
    "[Your Output]: \"\"\"\n",
    "\n",
    "\n",
    "LOGIC_CHAIN_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a rigorous biomedical expert. You need to construct logical reasoning chains based on the provided Original Text and Visual Evidence.\n",
    "\n",
    "Original Text:\n",
    "{context}\n",
    "\n",
    "Visual Evidence:\n",
    "{observation}\n",
    "\n",
    "Please integrate the Context and Visual Evidence to form detailed logical reasoning chains that lead to conclusions.\n",
    "Requirements:\n",
    "- Each independent research in the Context should correspond to a separate logical reasoning chain.\n",
    "- Each logical reasoning chain should follow the logic:\n",
    "  Research Context -> Experiments -> Conclusion\n",
    "- Each Experiment should follow the logic:\n",
    "  Experimental Setting -> Experiment Goal -> Visual Phenomenon -> Interpretation -> Sub-Conclusion\n",
    "    - Experimental Setting: Describe the experimental setup, including materials, methods, and conditions.\n",
    "    - Experiment Goal: Purpose of the experiment.\n",
    "    - Visual Phenomenon: Specific **visual** observations from the experiment, not interpretations.\n",
    "    - Interpretation: Scientific explanation of the visual phenomenon.\n",
    "    - Sub-Conclusion: Conclusion drawn from the interpretation, related to the final conclusion.\n",
    "- If a visual phenomenon of experiment is mentioned in the Visual Evidence, mark which image it corresponds to in the format [Image X].\n",
    "- Some experiments may not have any visual phenomenon in the Visual Evidence. There are two cases:\n",
    "    - The Context provides the visual phenomenon directly. In this case, provide the visual phenomenon and mark it as [Context].\n",
    "    - The visual phenomenon is missing. In this case, provide [Missing] as the visual phenomenon.\n",
    "- Avoid precise numerical measurement data unless exactly the same numbers are present in Experimental Setting.\n",
    "- The process of achieving the conclusion should include all necessary intermediate sub-conclusions and corresponding experiments.\n",
    "- Each logical reasoning chain should end with a clear conclusion.\n",
    "- If a certain experiment has no contribution to the final conclusion, it should be omitted from the whole logical reasoning chain.\n",
    "Output Format:\n",
    "Provide the logical reasoning chains in JSON format as a list of objects with the following structure:\n",
    "```json\n",
    "[\n",
    "  {{\n",
    "    \"research_context\": \"Description of the research context.\",\n",
    "    \"experiments\": [\n",
    "      {{\n",
    "        \"experimental_setting\": \"Description of the experimental setting.\",\n",
    "        \"experiment_goal\": \"Description of the experiment goal.\",\n",
    "        \"visual_phenomenon\": \"Visual phenomenon details with [Image X] or [Context] or [Missing].\",\n",
    "        \"interpretation\": \"Interpretation of the visual phenomenon.\",\n",
    "        \"sub_conclusion\": \"Conclusion drawn from the interpretation, related to the final conclusion.\"\n",
    "      }},\n",
    "      ...\n",
    "    ],\n",
    "    \"reasoning\": {{\n",
    "      \"intermediate_inferences\": [\n",
    "        {{\n",
    "          \"sub_conclusion\": \"Description of the intermediate inference.\",\n",
    "          \"based_on_experiments\": [Indices of experiments contributing to this inference]\n",
    "        }},\n",
    "        ...\n",
    "      ],\n",
    "      \"content\": \"Detailed reasoning process leading to the conclusion.\",\n",
    "      \"conclusion\": \"Final conclusion derived from the reasoning.\"\n",
    "    }}\n",
    "  }}\n",
    "]\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "OPEN_ENDED_QA_GENERATION_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a biomedical expert.\n",
    "You are given a logic chain, and you need to generate a exam question to test students' comprehension of the logic chain.\n",
    "\n",
    "Logic Chain:\n",
    "{logic_chain}\n",
    "\n",
    "Some extra information that may help you:\n",
    "Visual Evidence:\n",
    "{visual_evidence}\n",
    "\n",
    "Original Text:\n",
    "{original_text}\n",
    "\n",
    "The questions is expected to be hard, requiring both accurate observations and deep understanding of the logic chain. This includes the following aspects:\n",
    "- For the answer:\n",
    "    - The question should be open-ended.\n",
    "    - The answer should contain the whole logical reasoning chain above.\n",
    "\n",
    "- For the information provided in the question:\n",
    "    - The Research Context should be provided.\n",
    "    - The Setting of each Experiment should be provided.\n",
    "    - The Goal of each Experiment should NEVER be provided.\n",
    "    - For Visual Phenomenon of each Experiment:\n",
    "        - If at least one visual phenomenon of the experiment is mentioned in the Visual Evidence, do NOT provide the Visual Phenomenon or Result. I.e. sentence like \"[Image X] shows ...\" should NEVER appear in the question.\n",
    "          - Only one EXEMPT: If Visual Phenomenon contains Scale Bars, mention the scale ratio in the question.\n",
    "        - If all visual phenomena of the experiment are provided in Context, provide the Visual Phenomenon. Do NOT provide the Result.\n",
    "        - If the visual phenomenon is marked as Missing, provide the experiment result instead.\n",
    "    - The direct result (NOT further Interpretation or Sub-conclusion) of each experiment should be provided only if the Visual Phenomenon is marked as Missing.\n",
    "    - The Interpretation and Sub-Conclusion of each Experiment should NEVER be provided.\n",
    "    - Intermediate Inferences should NEVER be provided.\n",
    "    - Reasoning from Intermediate Inferences to Conclusion should NEVER be provided.\n",
    "    - The Conclusion should NEVER be provided.\n",
    "\n",
    "- For how to ask the question:\n",
    "    - The question should not easily guide students to the answer. That is:\n",
    "        - The question should not give any clues about how to reason to the answer.\n",
    "        - The question should not give away intermediate steps or conclusions.\n",
    "\n",
    "For example:\n",
    "The logic chain is:\n",
    "\n",
    "Research Context RC\n",
    "Experiment E1:\n",
    "  Setting: S1\n",
    "  Visual Phenomenon: P1 [Image X]\n",
    "  Interpretation: I1\n",
    "  Sub-Conclusion: SC1\n",
    "Experiment E2:\n",
    "  Setting: S2\n",
    "  Visual Phenomenon: P2 [Context]\n",
    "  Interpretation: I2\n",
    "  Sub-Conclusion: SC2\n",
    "Experiment E3:\n",
    "  Setting: S3\n",
    "  Visual Phenomenon: [Missing]\n",
    "  Interpretation: I3\n",
    "  Sub-Conclusion: SC3\n",
    "Final Reasoning:\n",
    "  Based on SC1, SC2 and SC3, we conclude Conclusion C.\n",
    "\n",
    "Do NOT ask:\n",
    "- [BAD CASE] \"How is conclusion C derived?\" (gives away the conclusion)\n",
    "- [BAD CASE] \"Research RC, conducted experiment E1, setting S1, observed P1, ...\" (gives away visible phenomenon in the images)\n",
    "- [BAD CASE] \"Research RC, ..., conducted experiment E2, setting S2, result R2, ...\" (gives away experiment result where phenomenon is provided in Context)\n",
    "- [BAD CASE] \"Research RC, ..., conducted experiment E3, interpretation I3, ...\" (gives away interpretation)\n",
    "- [BAD CASE] \"Research RC, conducted experiment E1, (Did not provide S1), ...\" (misses the setup of an experiment)\n",
    "- [BAD CASE] \"Research RC, ..., conducted experiment E2, setting S2, (Did not provide P2), ...\" (misses the visual phenomenon that is not provided in the images but provided in Context)\n",
    "- [BAD CASE] \"Research RC, ..., conducted experiment E3, setting S3, (Neither phenomenon or direct result), ...\" (misses the direct result of an experiment where the visual phenomenon is marked as Missing)\n",
    "- [BAD CASE] \"Based on SC1, SC2 and SC3, what is the conclusion?\" (gives away intermediate inferences)\n",
    "Ask instead:\n",
    "[GOOD CASE] \"Research RC, conducted experiment E1, setting S1; conducted experiment E2, setting S2, observed P2; conducted experiment E3, setting S3, got result R3. What can be concluded from these experiments?\"\n",
    "Note that you do not need to directly ask \"Please give a detailed reasoning process\". A clever student should know to provide the reasoning process to reach the conclusion.\n",
    "\n",
    "\n",
    "Format your output as a JSON object with three fields: \"question\" and \"answer\", where \"question\" contains the generated question, \"answer\" and \"explanation\", where \"explanation\" explains how you generated this question-answer pair according to the requirements above.\n",
    "```json\n",
    "{{\n",
    "  \"explanation\": \"{{your explanation here}}\",\n",
    "  \"question\": \"{{our question here}}\",\n",
    "  \"answer\": \"{{your answer here}}\"\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "LOGIC_CHAIN_QC_1_TEMPLATE = \\\n",
    "\"\"\"\n",
    "You are an expert in biomedical reasoning and logic evaluation.\n",
    "Your task is to evaluate the integrity and coherence of a logic chain.\n",
    "The input is a structured list of strings representing the progression from experimental facts to intermediate inferences, and finally to a conclusion.\n",
    "\n",
    "# Input Data\n",
    "\n",
    "[Logic Chain] (The reasoning path to evaluate):\n",
    "{flattened_logic_chain}\n",
    "\n",
    "# Evaluation Criteria (1-5 Scale)\n",
    "\n",
    "1. Evidence Support Strength\n",
    "   Assess if the intermediate inferences provide sufficient and accurate support for the final reasoning content.\n",
    "   - Score 1 (Critical Fail): Contradictory or Unsupported. The final content makes claims that contradict the intermediate inferences or relies on evidence not present in the chain.\n",
    "   - Score 3 (Borderline): Weak or Partial Support. The final content is somewhat related but contains major leaps in logic or includes details not fully backed by the intermediate steps.\n",
    "   - Score 5 (Pass): Strong Support. The final content is a robust and accurate synthesis strictly derived from the provided intermediate inferences.\n",
    "\n",
    "2. Logical Flow and Coherence\n",
    "   Assess if the transition from Intermediate Inferences to the Final Conclusion is logically sound and seamless.\n",
    "   - Score 1 (Critical Fail): Fragmented or Disjointed. The logic jumps randomly; the connection between the inference layer and the conclusion layer is broken or nonsensical.\n",
    "   - Score 3 (Borderline): Rough or Repetitive. The flow is understandable but clunky, redundant, or requires the reader to guess the connection between steps.\n",
    "   - Score 5 (Pass): Seamless and Coherent. The reasoning flows naturally like a scientific argument; the conclusion feels like the inevitable result of the preceding steps.\n",
    "\n",
    "# Output Format (Strict JSON)\n",
    "\n",
    "You must return the result strictly in the following format:\n",
    "\n",
    "<scores>\n",
    "{{\n",
    "  \"Evidence Support Strength\": A,\n",
    "  \"Logical Flow and Coherence\": B\n",
    "}}\n",
    "</scores>\n",
    "\n",
    "<explanation>\n",
    "[Provide a brief explanation for your scoring. explicitly stating if there are logical gaps, contradictions, or if the chain is solid.]\n",
    "</explanation>\n",
    "\n",
    "(Where A, B are integer scores from 1 to 5)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "LOGIC_CHAIN_QC_2_TEMPLATE = \\\n",
    "\"\"\"\n",
    "You are an expert in biomedical text verification and fact-checking.\n",
    "Your task is to verify if the [Visual Phenomena] described in the logic chain are supported by the provided Source Data ([Observation] and [Context]).\n",
    "\n",
    "# Input Data\n",
    "\n",
    "[Observation] (Objective visual descriptions of the images):\n",
    "{Observation}\n",
    "\n",
    "[Context] (Background containing [Image] tags):\n",
    "{Context}\n",
    "\n",
    "[Visual Phenomena] (The descriptions extracted from the logic chain to be verified):\n",
    "{VisualPhenomena}\n",
    "\n",
    "# Evaluation Criteria (1-5 Scale)\n",
    "\n",
    "1. Source Grounding & Verification\n",
    "   Assess if every visual phenomenon listed in the Target is explicitly mentioned or clearly visible in the [Context] or [Observation].\n",
    "   - Score 1 (Critical Fail): Hallucination. The target describes features that are completely absent from both the Observation and Context, or contradicts them.\n",
    "   - Score 3 (Borderline): Partial Match. Some descriptions are supported, but others are missing source evidence, or the target adds significant details not found in the source.\n",
    "   - Score 5 (Pass): Fully Grounded. Every statement in the [Visual Phenomena] is directly supported by evidence found in the Source Observation or Source Context (textual descriptions of visual outcomes).\n",
    "\n",
    "# Output Format (Strict JSON)\n",
    "\n",
    "You must return the result strictly in the following format:\n",
    "\n",
    "<scores>\n",
    "{{\n",
    "  \"Source Grounding & Verification\": A\n",
    "}}\n",
    "</scores>\n",
    "\n",
    "<explanation>\n",
    "[Provide a brief explanation. If there is a hallucination or missing reference, explicitly quote the unsupported part.]\n",
    "</explanation>\n",
    "\n",
    "(Where A is an integer score from 1 to 5)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "LOGIC_CHAIN_QC_3_TEMPLATE = \\\n",
    "\"\"\"\n",
    "You are an expert in evaluating question-answering logic.\n",
    "Your task is to verify if the provided Conclusion effectively answers or corresponds to the specific Question asked.\n",
    "\n",
    "# Input Data\n",
    "\n",
    "Question:\n",
    "{Question}\n",
    "\n",
    "Observation (Visual Evidence containing scale info):\n",
    "{Observation}\n",
    "\n",
    "Logic Chain:\n",
    "{LogicChain}\n",
    "\n",
    "Conclusion (Derived from Logic Chain):\n",
    "{Conclusion}\n",
    "\n",
    "# Evaluation Criteria (1-5 Scale)\n",
    "\n",
    "1. Question-Conclusion Alignment\n",
    "   Assess if the Conclusion directly addresses the core inquiry of the Question.\n",
    "   - Score 1 (Fail): The conclusion is irrelevant, unrelated, or contradicts the premise of the question. It does not provide an answer.\n",
    "   - Score 3 (Passable): The conclusion is related and provides a partial answer, but may be slightly tangential or misses the specific format requested.\n",
    "   - Score 5 (Pass): The conclusion provides a clear, logical, and direct answer to the question. It functions effectively as the final output.\n",
    "\n",
    "2. Scale/Legend Consistency Check\n",
    "Check if the problem statement lacks a scale/legend, but the observation results, reasoning content, and conclusion clearly include scale numbers or scale information.\n",
    "- Score 1 point (Serious Failure): The problem statement lacks a scale/legend, but the observation results contain explicit scale numbers (e.g., \"50 nm,\" \"scale\"), and the reasoning content utilizes this scale information from the observation.\n",
    "- Score 5 points (Pass): The problem statement and observation results are consistent; either both include a scale/legend, or neither includes scale-related information. If the problem statement includes scale-related information, but the conclusion and reasoning content do not use it, it is not considered an error.\n",
    "\n",
    "3. Reasoning Validity \n",
    "   Assess if the Logic Chain steps contains excessive speculation or hallucinations not supported by the Observation.\n",
    "   - Score 1 (Critical Fail): Given ONLY Research Context, Experimental Settings, and Visual Phenomenon, the \"inference\", \"sub_conclusion\", \"content\", \"conclusion\" parts contains details impossible to know.\n",
    "   - Score 5 (Pass): Given ONLY Research Context, Experimental Settings, and Visual Phenomenon, the \"inference\", \"sub_conclusion\", \"content\", \"conclusion\" parts are all supported without any hallucination.\n",
    "\n",
    "# Output Format (Strict JSON)\n",
    "\n",
    "You must return the result strictly in the following format:\n",
    "\n",
    "<scores>\n",
    "{{\n",
    "  \"Question-Conclusion Alignment\": A,\n",
    "  \"Scale/Legend Consistency Check\" : B,\n",
    "  \"Reasoning Validity\" : C\n",
    "}}\n",
    "</scores>\n",
    "\n",
    "<explanation>\n",
    "[Briefly explain why the conclusion satisfies or fails to answer the question.]\n",
    "</explanation>\n",
    "\n",
    "\n",
    "(Where A, B, C is an integer score from 1 to 5)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f57553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_content(prompt, images):\n",
    "    image_list = images or [] \n",
    "    content = [\n",
    "        {\"type\": \"image_url\", \"image_url\": img_url}\n",
    "        for img_url in image_list\n",
    "    ] + [\n",
    "        {\"type\": \"text\", \"text\": prompt}\n",
    "    ]\n",
    "    return content\n",
    "\n",
    "def openai_pack_content(prompt, images):\n",
    "    image_list = images or []\n",
    "    content = [\n",
    "        {\"type\": \"image_url\", \"image_url\": {\n",
    "            \"url\": img_url,\n",
    "            \"detail\": \"auto\"\n",
    "        }}\n",
    "        for img_url in image_list\n",
    "    ] + [\n",
    "        {\"type\": \"text\", \"text\": prompt}\n",
    "    ]\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1adbd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_qa_output(output_str):\n",
    "    output_str = output_str.strip()\n",
    "    if output_str.startswith(\"```json\") and output_str.endswith(\"```\"):\n",
    "        output_str = output_str[len(\"```json\"): -len(\"```\")].strip()\n",
    "    try:\n",
    "        qa_list = json.loads(output_str)\n",
    "        return qa_list\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decoding error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecfcd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_quality_score(quality_check_output_str):\n",
    "    score = None\n",
    "    explanation = None\n",
    "    \n",
    "\n",
    "    score_start = quality_check_output_str.index(\"<scores>\") + len(\"<scores>\")\n",
    "    score_end = quality_check_output_str.index(\"</scores>\")\n",
    "    score_json_str = quality_check_output_str[score_start:score_end].strip()\n",
    "    score = json.loads(score_json_str)\n",
    "\n",
    "\n",
    "    explanation_start = quality_check_output_str.index(\"<explanation>\") + len(\"<explanation>\")\n",
    "    explanation_end = quality_check_output_str.index(\"</explanation>\")\n",
    "    explanation = quality_check_output_str[explanation_start:explanation_end].strip()\n",
    "\n",
    "    return score, explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1be711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a436200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer_format_rule(answer_string: str):\n",
    "    required_tags = {\n",
    "        \"description\": [\"<description>\", \"</description>\"],\n",
    "        \"reason\": [\"<reason>\", \"</reason>\"],\n",
    "        \"boxed\": [\"\\\\boxed{\", \"}\"]\n",
    "    }\n",
    "    extracted_contents = {}\n",
    "    for tag, (start_tag, end_tag) in required_tags.items():\n",
    "        start_index = answer_string.find(start_tag)\n",
    "        end_index = answer_string.find(end_tag)\n",
    "        if start_index == -1 or end_index == -1 or start_index >= end_index:\n",
    "            return False, None\n",
    "        extracted_contents[tag] = answer_string[start_index + len(start_tag): end_index].strip()\n",
    "    return True, extracted_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b1e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alter_tags(answer_string: str):\n",
    "    answer_string = answer_string.replace(\"<description>\", \"\\\\<description\\\\>\")\n",
    "    answer_string = answer_string.replace(\"</description>\", \"\\\\</description\\\\>\")\n",
    "    answer_string = answer_string.replace(\"<reason>\", \"\\\\<reason\\\\>\")\n",
    "    answer_string = answer_string.replace(\"</reason>\", \"\\\\</reason\\\\>\")\n",
    "    return answer_string\n",
    "\n",
    "def format_data_md(results):\n",
    "    md_lines = []\n",
    "    for idx, res in enumerate(results):\n",
    "        if res[\"qa_pair\"] is None:\n",
    "            continue\n",
    "        md_lines.append(f\"## Sample {idx + 1}\\n\")\n",
    "        md_lines.append(f\"**Original Sample Index:** {res['index']}\\n\")\n",
    "        md_lines.append(\"**Context:**\\n\")\n",
    "        md_lines.append(f\"{res['context']}\\n\")\n",
    "        md_lines.append(\"**Image Captions:**\\n\")\n",
    "        for idx, caption in enumerate(res[\"image_captions\"], 1):\n",
    "            md_lines.append(f\"- Image {idx}: {caption}\\n\")\n",
    "\n",
    "        keyword_result = res.get('keyword_category_result', 'N/A')\n",
    "        md_lines.append(\"**I. Thematic Classification and Keywords:**\\n\")\n",
    "        md_lines.append(f\"> {keyword_result}\\n\")\n",
    "        md_lines.append(\"---\\n\") \n",
    "\n",
    "        md_lines.append(\"**Question-Answer Pair:**\\n\")\n",
    "        image_indices = res['qa_pair'].get('image_indices', [])\n",
    "        md_lines.append(f\"**Image Indices Used (1-indexed):** {image_indices}\\n\")\n",
    "        md_lines.append(f\"**Question:**\\n{res['qa_pair']['question']}\\n\")\n",
    "        md_lines.append(f\"**Answer:**\\n{alter_tags(res['qa_pair']['answer'])}\\n\")\n",
    "        md_lines.append(\"**Format Check Result:**\\n\")\n",
    "        md_lines.append(f\"{res['format_check']}\\n\")\n",
    "        md_lines.append(\"**Quality Scores:**\\n\")\n",
    "        md_lines.append(f\"```json\\n{json.dumps(res['quality_score'], indent=2)}\\n```\\n\")\n",
    "        md_lines.append(\"**Quality Explanation:**\\n\")\n",
    "        md_lines.append(f\"{alter_tags(res['quality_explanation'])}\\n\")\n",
    "        md_lines.append(\"---\\n\")\n",
    "    return \"\\n\".join(md_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328b8bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def build_caption_with_id(img_info):\n",
    "    caption = img_info.get(\"caption\", \"\")\n",
    "    fig_id = img_info.get(\"fig_id\", \"\")\n",
    "    sub_label = img_info.get(\"subfig_label\", \"\")\n",
    "\n",
    "\n",
    "    if fig_id:\n",
    "\n",
    "        prefix = f\"This is {fig_id}{sub_label}. \"\n",
    "        return prefix + caption\n",
    "    else:\n",
    "        return caption\n",
    "\n",
    "\n",
    "\n",
    "def extract_specific_context(item, target_indices):\n",
    "    captions_list = item.get(\"context_enhanced_captions\", [])\n",
    "    summary_data = item.get(\"context_enhanced_summary\", {})\n",
    "    \n",
    "    obs_parts = []\n",
    "    int_parts = []\n",
    "    \n",
    "    for entry in captions_list:\n",
    "        idx = entry.get(\"image_index\")\n",
    "        if idx in target_indices:\n",
    "            obs = entry.get(\"observation\", \"\")\n",
    "            if obs: obs_parts.append(f\"[Image {idx}]: {obs}\")\n",
    "            interp = entry.get(\"interpretation\", \"\")\n",
    "            if interp: int_parts.append(f\"[Image {idx}]: {interp}\")\n",
    "    \n",
    "    if summary_data.get(\"observation_summary\"):\n",
    "        obs_parts.append(f\"[observation_summary]: {summary_data['observation_summary']}\")\n",
    "    if summary_data.get(\"interpretation_summary\"):\n",
    "        int_parts.append(f\"[interpretation_summary]: {summary_data['interpretation_summary']}\")\n",
    "        \n",
    "    return \"\\n\".join(obs_parts), \"\\n\".join(int_parts)\n",
    "\n",
    "\n",
    "def extract_interpretation_text(item, target_indices=None):\n",
    "    captions_list = item.get(\"context_enhanced_captions\", [])\n",
    "    summary_data = item.get(\"context_enhanced_summary\", {})\n",
    "    int_summary = summary_data.get(\"interpretation_summary\", \"\")\n",
    "    combined_int_parts = []\n",
    "    for entry in captions_list:\n",
    "        idx = entry.get(\"image_index\")\n",
    "        if target_indices and idx not in target_indices:\n",
    "            continue\n",
    "        int_text = entry.get(\"interpretation\", \"\")\n",
    "        if int_text and int_text != \"Not found\":\n",
    "            combined_int_parts.append(f\"[Image {idx} Interpretation]: {int_text}\")\n",
    "    if int_summary:\n",
    "        combined_int_parts.append(f\"[Overall Summary]: {int_summary}\")\n",
    "    return \"\\n\".join(combined_int_parts)\n",
    "\n",
    "\n",
    "def format_background_intro(theme_data, target_indices):\n",
    "    if not theme_data:\n",
    "        return \"\"\n",
    "    \n",
    "\n",
    "    exp_bg =theme_data.get(\"Experimental background\", \"N/A\")\n",
    "    \n",
    "\n",
    "    all_themes = theme_data.get(\"Image Settings\", {})\n",
    "    \n",
    "    selected_themes = {}\n",
    "    indices_to_check = target_indices if target_indices else [int(k.replace(\"Image \", \"\")) for k in all_themes.keys() if \"Image\" in k]\n",
    "    \n",
    "    for idx in indices_to_check:\n",
    "        key = f\"Image {idx}\"\n",
    "        if key in all_themes:\n",
    "            selected_themes[key] = all_themes[key]\n",
    "            \n",
    "\n",
    "    background_dict = {\n",
    "        \"Experimental background\": exp_bg,\n",
    "        \"Image Settings\": selected_themes\n",
    "    }\n",
    "    \n",
    "    return json.dumps(background_dict, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe8c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import asyncio\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from openai import AsyncOpenAI, APIConnectionError, InternalServerError\n",
    "from asyncio import as_completed\n",
    "from tqdm import tqdm\n",
    "import httpx\n",
    "\n",
    "import logging\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "dashscope_api_key = os.getenv(\"DASHSCOPE_API_KEY\") or \"1\"\n",
    "vl_model = \"qwen3-vl-plus\"\n",
    "text_model = \"qwen-plus\"\n",
    "\n",
    "local_vl_api_key = \"rk_test_wA9qF7sB2mXpLcN8zYtVdRkHjMnPqWsT3\"\n",
    "local_vl_model = \"qwen3_vl_235b_instruct\"\n",
    "local_text_api_key = \"sk_live_123456789\"\n",
    "local_text_model = \"qwen3_235b_instruct\"\n",
    "\n",
    "\n",
    "# --- Async Clients ---\n",
    "dashscope_client = AsyncOpenAI(\n",
    "    api_key=dashscope_api_key,\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    timeout=120.0\n",
    ")\n",
    "\n",
    "local_vl_client = AsyncOpenAI(\n",
    "    api_key=local_vl_api_key,\n",
    "    base_url=\"http://183.221.202.124:18080/v1\",\n",
    "    timeout=120.0\n",
    ")\n",
    "\n",
    "local_text_client = AsyncOpenAI(\n",
    "    api_key=local_text_api_key,\n",
    "    base_url=\"http://8.152.194.113:8091/v1\",\n",
    "    timeout=120.0\n",
    ")\n",
    "\n",
    "\n",
    "# Configuration for Text Model (e.g., Gemini-2.0-flash via OpenAI compat)\n",
    "local_text_api_key = \"xxx\"\n",
    "local_text_model = \"xxx\"\n",
    "local_text_client = AsyncOpenAI(\n",
    "    api_key=local_text_api_key,\n",
    "    base_url=\"xxx\",\n",
    "    timeout=120.0\n",
    ")\n",
    "\n",
    "\n",
    "# Configuration for VL Model\n",
    "local_vl_api_key = \"xxx\"\n",
    "local_vl_model = \"xxx\"\n",
    "local_vl_client = AsyncOpenAI(\n",
    "    api_key=local_vl_api_key,\n",
    "    base_url=\"xxx\", \n",
    "    timeout=120.0\n",
    ")\n",
    "\n",
    "async def get_response_async(prev_messages,\n",
    "                             next_content,\n",
    "                             model,\n",
    "                             client,\n",
    "                             tools=None,\n",
    "                             max_retries=3):\n",
    "\n",
    "    # Handle content type automatically\n",
    "    if isinstance(next_content, str):\n",
    "        user_content = next_content  # For text-only\n",
    "    else:\n",
    "        user_content = next_content  # For multimodal\n",
    "\n",
    "    messages = prev_messages + [{\"role\": \"user\", \"content\": user_content}]\n",
    "    MAX_TOKENS_LIMIT = 4096 \n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            reasoning_content = \"\"\n",
    "            answer_content = \"\"\n",
    "            tool_info = []\n",
    "            is_answering = False\n",
    "\n",
    "            if tools is not None:\n",
    "                response = await client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    tools=tools,\n",
    "                    parallel_tool_calls=True,\n",
    "                    stream=True,\n",
    "                    max_tokens=MAX_TOKENS_LIMIT\n",
    "                )\n",
    "            else:\n",
    "                response = await client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    stream=True,\n",
    "                    max_tokens=MAX_TOKENS_LIMIT\n",
    "                )\n",
    "\n",
    "            async for chunk in response:\n",
    "                if chunk.choices:\n",
    "                    delta = chunk.choices[0].delta\n",
    "                    # Extract reasoning content if supported by the model\n",
    "                    if hasattr(delta, 'reasoning_content') and delta.reasoning_content != None:\n",
    "                        reasoning_content += delta.reasoning_content\n",
    "                    else:\n",
    "                        if not is_answering:\n",
    "                            is_answering = True\n",
    "                        if delta.content is not None:\n",
    "                            answer_content += delta.content\n",
    "                        \n",
    "                        # Handle tool calls in stream\n",
    "                        if delta.tool_calls is not None:\n",
    "                            for tool_call in delta.tool_calls:\n",
    "                                index = tool_call.index\n",
    "                                while len(tool_info) <= index:\n",
    "                                    tool_info.append({})\n",
    "                                if tool_call.id:\n",
    "                                    tool_info[index]['id'] = tool_info[index].get('id', '') + tool_call.id\n",
    "                                if tool_call.function and tool_call.function.name:\n",
    "                                    tool_info[index]['name'] = tool_info[index].get('name', '') + tool_call.function.name\n",
    "                                if tool_call.function and tool_call.function.arguments:\n",
    "                                    tool_info[index]['arguments'] = tool_info[index].get('arguments', '') + tool_call.function.arguments\n",
    "                                if tool_call.type:\n",
    "                                    tool_info[index]['type'] = tool_call.type\n",
    "\n",
    "            # Fallback for models that wrap reasoning in <think> tags\n",
    "            if not reasoning_content:\n",
    "                if answer_content.startswith(\"<think>\"):\n",
    "                    end_think_idx = answer_content.find(\"</think>\")\n",
    "                    if end_think_idx != -1:\n",
    "                        reasoning_content = answer_content[len(\"<think>\"):end_think_idx]\n",
    "                        answer_content = answer_content[end_think_idx + len(\"</think>\"):]\n",
    "\n",
    "            new_message = {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": answer_content,\n",
    "            }\n",
    "            \n",
    "            if len(tool_info) > 0:\n",
    "                tool_calls = [{\n",
    "                    \"id\": tool_call[\"id\"],\n",
    "                    \"function\": {\n",
    "                        \"name\": tool_call[\"name\"],\n",
    "                        \"arguments\": tool_call[\"arguments\"]\n",
    "                    },\n",
    "                    \"type\": tool_call[\"type\"],\n",
    "                    \"index\": i\n",
    "                } for i, tool_call in enumerate(tool_info)]\n",
    "                new_message[\"tool_calls\"] = tool_calls\n",
    "            \n",
    "            messages.append(new_message)\n",
    "\n",
    "            return {\n",
    "                \"content\": answer_content,\n",
    "                \"reasoning_content\": reasoning_content,\n",
    "                \"usage\": None,  \n",
    "                \"prev_messages\": messages,\n",
    "                \"tool_info\": tool_info\n",
    "            }\n",
    "\n",
    "        except (APIConnectionError, InternalServerError) as e:\n",
    "            print(f\"--- [Retryable Error] (Attempt {attempt + 1}/{max_retries}): {e}\")\n",
    "            if attempt == max_retries - 1: raise e\n",
    "            await asyncio.sleep(5)\n",
    "\n",
    "        except Exception as e:\n",
    "            error_str = str(e).lower()\n",
    "            # Handle specific network/stream cutoff issues\n",
    "            if any(msg in error_str for msg in [\"incomplete chunked read\", \"peer closed connection\", \"connection closed\"]):\n",
    "                print(f\"--- [Network/Server Cutoff] (Attempt {attempt + 1}/{max_retries}): {e}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    print(\"--- Max retries reached for cutoff error.\")\n",
    "                    raise e\n",
    "                await asyncio.sleep(10)  \n",
    "            else:\n",
    "                print(f\"--- [Fatal Error]: {e}\")\n",
    "                raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd3882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "INPUT_FILE = 'sorce_data.json'\n",
    "OUTPUT_FILE_STEP0 = \"step0_filtered_data.json\"\n",
    "OUTPUT_FILE_DISCARDED = \"step0_discarded_data.json\"\n",
    "CONCURRENT_LIMIT_STEP0 = 10\n",
    "\n",
    "if os.path.exists(INPUT_FILE):\n",
    "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Successfully loaded {len(data)} items from {INPUT_FILE}\")\n",
    "    RAW_DATA_SOURCE = data\n",
    "else:\n",
    "    print(f\"Error: File {INPUT_FILE} not found.\")\n",
    "    RAW_DATA_SOURCE = []\n",
    "\n",
    "\n",
    "async def check_biomedical_async(sample):\n",
    "    try:\n",
    "        back_info = sample.get(\"back_info\", \"\")\n",
    "        if not back_info:\n",
    "             text_list = sample.get(\"text_list\", [])\n",
    "             back_info = \" \".join([t for t in text_list if isinstance(t, str)])\n",
    "        \n",
    "        context_for_judge = back_info\n",
    "        prompt = BIOMED_CHECK_PROMPT.format(context=context_for_judge) \n",
    "\n",
    "        response = await get_response_async(\n",
    "            [], prompt, local_text_model, local_text_client\n",
    "        )\n",
    "        content = response[\"content\"].strip()\n",
    "\n",
    "        if content.startswith(\"```json\"): content = content[7:].strip()\n",
    "        if content.endswith(\"```\"): content = content[:-3].strip()\n",
    "        \n",
    "        is_biomedical = False\n",
    "        try:\n",
    "            res_json = json.loads(content)\n",
    "            is_biomedical = res_json.get(\"is_biomedical\", False)\n",
    "        except:\n",
    "            if \"true\" in content.lower():\n",
    "                is_biomedical = True\n",
    "\n",
    "        if is_biomedical:\n",
    "            raw_image_info = sample.get(\"image_info\", [])\n",
    "            formatted_captions = []\n",
    "            \n",
    "            for i, img in enumerate(raw_image_info):\n",
    "                formatted_captions.append({\n",
    "                    \"image_index\": i + 1,\n",
    "                    \"caption\": img.get(\"caption\", \"\")\n",
    "                })\n",
    "\n",
    "            lightweight_sample = {\n",
    "                \"original_sample_index\": sample.get(\"original_sample_index\"),\n",
    "                \"text_list\": sample.get(\"text_list\", []),\n",
    "                \"back_info\": sample.get(\"back_info\", \"\"),\n",
    "                \"image_captions\": formatted_captions\n",
    "            }\n",
    "\n",
    "            return {\n",
    "                \"status\": \"valid\",\n",
    "                \"data\": lightweight_sample\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"status\": \"filtered\",\n",
    "                \"original_sample_index\": sample.get(\"original_sample_index\"),\n",
    "                \"context_preview\": context_for_judge[:100], \n",
    "                \"llm_raw_response\": content\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"error\": str(e), \"original_sample_index\": sample.get(\"original_sample_index\")}\n",
    "\n",
    "async def main_step0_filter():\n",
    "    semaphore = asyncio.Semaphore(CONCURRENT_LIMIT_STEP0)\n",
    "    tasks = []\n",
    "    print(f\"Step 0: Processing {len(RAW_DATA_SOURCE)} items...\")\n",
    "\n",
    "    for sample in RAW_DATA_SOURCE:\n",
    "        async def wrapped(s):\n",
    "            async with semaphore:\n",
    "                return await check_biomedical_async(s)\n",
    "        tasks.append(wrapped(sample))\n",
    "\n",
    "    results = await tqdm_asyncio.gather(*tasks)\n",
    "\n",
    "    valid_samples = []\n",
    "    discarded_samples = [] \n",
    "    \n",
    "    for res in results:\n",
    "        if res[\"status\"] == \"valid\":\n",
    "            valid_samples.append(res[\"data\"])\n",
    "        elif res[\"status\"] == \"filtered\":\n",
    "            discarded_samples.append(res) \n",
    "            \n",
    "    with open(OUTPUT_FILE_STEP0, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(valid_samples, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    with open(OUTPUT_FILE_DISCARDED, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(discarded_samples, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\nProcessing Complete\")\n",
    "    print(f\"Total Input: {len(RAW_DATA_SOURCE)}\")\n",
    "    print(f\"Kept (Valid): {len(valid_samples)}\")\n",
    "    print(f\"Discarded: {len(discarded_samples)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main_step0_filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063824b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "INPUT_FILE_FROM_STEP0 = \"step0_filtered_data.json\"\n",
    "OUTPUT_FILE_STEP1 = \"step1_keywords_output.json\" \n",
    "NUM_SAMPLES_TO_TEST = 200\n",
    "CONCURRENT_LIMIT_STEP1 = 5 \n",
    "\n",
    "async def extract_keywords_from_filtered_async(sample):\n",
    "    orig_idx = sample.get(\"original_sample_index\", \"N/A\")\n",
    "    try:\n",
    "        context = sample.get(\"context\", \"\")\n",
    "        image_captions = sample.get(\"image_captions\", [])\n",
    "        if not context:\n",
    "            text_list = sample.get(\"text_list\", [])\n",
    "            modified_text_list = []\n",
    "            image_insert_counter = 0 \n",
    "            num_available_images = len(image_captions)\n",
    "            if not text_list:\n",
    "                processed_text = \"\"\n",
    "            else:\n",
    "                for text in text_list:\n",
    "                    has_images_left = (image_insert_counter < num_available_images)\n",
    "                    if text == \"\" and has_images_left:\n",
    "                        modified_text_list.append(f\" [Image {image_insert_counter + 1}]\")\n",
    "                        image_insert_counter += 1\n",
    "                    elif isinstance(text, str) and text.startswith(\")\") and has_images_left:\n",
    "                        modified_text_list.append(f\" [Image {image_insert_counter + 1}]{text}\")\n",
    "                        image_insert_counter += 1\n",
    "                    else:\n",
    "                        modified_text_list.append(str(text))\n",
    "                processed_text = \"\".join(modified_text_list)\n",
    "            context = processed_text\n",
    "\n",
    "        formatted_captions_for_llm = []\n",
    "        for img_item in image_captions:\n",
    "            idx = img_item.get(\"image_index\", \"?\")\n",
    "            cap = img_item.get(\"caption\", \"No caption\")\n",
    "            formatted_captions_for_llm.append(f\"Image {idx}: {cap}\")\n",
    "\n",
    "        captions_str_for_prompt = json.dumps(formatted_captions_for_llm, ensure_ascii=False, indent=2)\n",
    "        kw_prompt = KEYWORD_Category_PROMPT_TEMPLATE.format(\n",
    "            context=context, \n",
    "            image_caption=captions_str_for_prompt \n",
    "        )\n",
    "        response = await get_response_async([], kw_prompt, local_text_model, local_text_client)\n",
    "        keywords = response[\"content\"].strip()\n",
    "        return {\n",
    "            \"status\": \"success\", \n",
    "            \"original_sample_index\": orig_idx,\n",
    "            \"context\": context,  \n",
    "            \"image_captions\": image_captions, \n",
    "            \"extracted_keywords\": keywords,\n",
    "            \"back_info\": sample.get(\"back_info\", \"\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"failed\", \"original_sample_index\": orig_idx, \"error\": str(e)}\n",
    "\n",
    "async def main_keyword_extraction_step1():\n",
    "    if not os.path.exists(INPUT_FILE_FROM_STEP0):\n",
    "        print(f\"[Error] Step 0 file not found: {INPUT_FILE_FROM_STEP0}\")\n",
    "        return\n",
    "    with open(INPUT_FILE_FROM_STEP0, \"r\", encoding=\"utf-8\") as f:\n",
    "        filtered_data = json.load(f)\n",
    "    if not filtered_data:\n",
    "        print(\"[Warning] Data empty.\")\n",
    "        return\n",
    "    test_data = filtered_data[:NUM_SAMPLES_TO_TEST]\n",
    "    print(f\"Step 1: Processing {len(test_data)} items...\")\n",
    "    semaphore = asyncio.Semaphore(CONCURRENT_LIMIT_STEP1) \n",
    "    tasks = []\n",
    "    for sample in test_data:\n",
    "        async def wrapped(s):\n",
    "            async with semaphore:\n",
    "                return await extract_keywords_from_filtered_async(s)\n",
    "        tasks.append(wrapped(sample))\n",
    "    results = await tqdm_asyncio.gather(*tasks)\n",
    "    final_output = []\n",
    "    success_count = 0\n",
    "    for res in results:\n",
    "        if res[\"status\"] == \"success\":\n",
    "            final_output.append({\n",
    "                \"original_sample_index\": res[\"original_sample_index\"],\n",
    "                \"context\": res[\"context\"],\n",
    "                \"image_captions\": res[\"image_captions\"],\n",
    "                \"extracted_keywords\": res[\"extracted_keywords\"],\n",
    "                \"back_info\": res[\"back_info\"]\n",
    "            })\n",
    "            success_count += 1\n",
    "        else:\n",
    "            print(f\"[ID: {res.get('original_sample_index')}] Failed: {res.get('error')}\")\n",
    "    if final_output:\n",
    "        with open(OUTPUT_FILE_STEP1, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(final_output, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"\\n[SUCCESS] Completed. Success: {success_count}/{len(test_data)} | Saved to: {OUTPUT_FILE_STEP1}\")\n",
    "\n",
    "await main_keyword_extraction_step1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9bb96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "# ================= Configuration =================\n",
    "INPUT_FILE = \"step1_keywords_output.json\"\n",
    "OUTPUT_FILE = \"step2_distilled_output.json\"\n",
    "MAX_CONCURRENT_TASKS = 20\n",
    "\n",
    "def should_skip_distillation(background_text: str) -> bool:\n",
    "    if not background_text:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "async def process_single_sample_background(result_item, client, model):\n",
    "    oid = result_item.get(\"original_sample_index\")\n",
    "    \n",
    "    if oid is None:\n",
    "        return oid, {\"status\": \"error\", \"error_message\": \"Missing 'original_sample_index'\"}\n",
    "\n",
    "    back_info = result_item.get(\"back_info\")\n",
    "    \n",
    "    if not back_info or should_skip_distillation(back_info):\n",
    "         return oid, {\"status\": \"skipped\", \"message\": \"Background content missing or empty\"}\n",
    "\n",
    "    word_count = len(back_info.split()) \n",
    "    THRESHOLD = 200 \n",
    "\n",
    "    if word_count < THRESHOLD:\n",
    "        return oid, {\n",
    "            \"status\": \"success\", \n",
    "            \"distilled_background\": back_info, \n",
    "            \"note\": \"Short background, used original.\"\n",
    "        }\n",
    "    \n",
    "    prompt = BACKGROUND_DISTILLATION_PROMPT_TEMPLATE.format(back_info=back_info)\n",
    "    \n",
    "    try:\n",
    "        response = await get_response_async(\n",
    "            prev_messages=[], \n",
    "            next_content=prompt, \n",
    "            model=model, \n",
    "            client=client\n",
    "        )\n",
    "        distilled_content = response['content'].strip()\n",
    "        \n",
    "        if distilled_content.startswith(\"```\"):\n",
    "            lines = distilled_content.split('\\n')\n",
    "            if len(lines) >= 3:\n",
    "                distilled_content = '\\n'.join(lines[1:-1]).strip()\n",
    "        \n",
    "        return oid, {\"status\": \"success\", \"distilled_background\": distilled_content}\n",
    "\n",
    "    except Exception as e:\n",
    "        return oid, {\"status\": \"error\", \"error_message\": str(e)}\n",
    "\n",
    "async def main_background_distillation(input_path, output_path, client, model):\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"[Error] Input file not found: {input_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Reading input file: {input_path} ...\")\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        current_results = json.load(f)\n",
    "    \n",
    "    print(f\"Load successful. Total items: {len(current_results)}\")\n",
    "    if not current_results: return\n",
    "\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_TASKS)\n",
    "\n",
    "    async def sem_task(item):\n",
    "        async with semaphore:\n",
    "            return await process_single_sample_background(item, client, model)\n",
    "\n",
    "    tasks = [sem_task(item) for item in current_results]\n",
    "    \n",
    "    print(f\"Starting distillation process...\")\n",
    "    results = await tqdm_asyncio.gather(*tasks)\n",
    "    \n",
    "    results_map = {oid: res for oid, res in results if oid is not None}\n",
    "    stats = {\"original\": 0, \"distilled\": 0, \"error\": 0, \"skipped\": 0}\n",
    "\n",
    "    for item in current_results:\n",
    "        oid = item.get(\"original_sample_index\")\n",
    "        \n",
    "        if oid in results_map:\n",
    "            res = results_map[oid]\n",
    "            if res['status'] == 'success':\n",
    "                item[\"distilled_background\"] = res['distilled_background']\n",
    "                if res.get(\"note\") == \"Short background, used original.\":\n",
    "                    stats[\"original\"] += 1\n",
    "                else:\n",
    "                    stats[\"distilled\"] += 1\n",
    "            elif res['status'] == 'skipped':\n",
    "                item[\"distilled_background\"] = \"SKIPPED_MISSING\"\n",
    "                stats[\"skipped\"] += 1\n",
    "            else:\n",
    "                item[\"distilled_background\"] = f\"ERROR: {res.get('error_message')}\"\n",
    "                stats[\"error\"] += 1\n",
    "        else:\n",
    "            item[\"distilled_background\"] = \"ERROR_ID_NOT_FOUND\"\n",
    "            stats[\"error\"] += 1\n",
    "\n",
    "    try:\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(current_results, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"\\n[SUCCESS] Processing complete. Saved to: {output_path}\")\n",
    "        print(f\"Stats: Kept Original: {stats['original']} | LLM Distilled: {stats['distilled']} | Errors: {stats['error']} | Skipped: {stats['skipped']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Failed to save file: {e}\")\n",
    "\n",
    "# ================= Execution =================\n",
    "await main_background_distillation(\n",
    "    INPUT_FILE, \n",
    "    OUTPUT_FILE, \n",
    "    local_text_client,   \n",
    "    local_text_model     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8296c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using other VL models here, please configure the corresponding settings and file names.\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import base64\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "INPUT_FILE = \"step0_filtered_data.json\"\n",
    "SOURCE_DATA_FILE = \"qa_generation_quickly.json\"\n",
    "OUTPUT_FILE = \"step2_model_enhanced_captions_qwenvl.json\"\n",
    "\n",
    "MAX_CONCURRENT_VLM_TASKS = 10\n",
    "\n",
    "async def call_vlm_model_async(image_data_base64, prompt_text):\n",
    "    try:\n",
    "        response = await local_vl_client.chat.completions.create(\n",
    "            model=local_vl_model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt_text},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data_base64}\"}}\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=512,\n",
    "            temperature=0.2\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"ERROR_VLM_CALL: {str(e)}\"\n",
    "\n",
    "def get_image_base64_from_source(img_info):\n",
    "    if img_info.get(\"image_base64\"):\n",
    "        return img_info[\"image_base64\"]\n",
    "    \n",
    "    local_path = img_info.get(\"local_path\")\n",
    "    if local_path and os.path.exists(local_path):\n",
    "        try:\n",
    "            with open(local_path, \"rb\") as f:\n",
    "                return base64.b64encode(f.read()).decode('utf-8')\n",
    "        except:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "async def process_single_image_vlm(img_info, semaphore):\n",
    "    caption = img_info.get(\"caption\", \"\")\n",
    "    base64_str = get_image_base64_from_source(img_info)\n",
    "    \n",
    "    if not base64_str:\n",
    "        return \"ERROR: Image data missing\"\n",
    "\n",
    "    prompt = VLM_PROMPT_TEMPLATE.format(caption=caption)\n",
    "    \n",
    "    async with semaphore:\n",
    "        return await call_vlm_model_async(base64_str, prompt)\n",
    "\n",
    "async def process_sample_by_id(lightweight_item, source_map, semaphore):\n",
    "    orig_id = lightweight_item.get(\"original_sample_index\")\n",
    "    raw_sample = source_map.get(orig_id)\n",
    "    \n",
    "    if not raw_sample:\n",
    "        raw_sample = source_map.get(str(orig_id))\n",
    "\n",
    "    if not raw_sample:\n",
    "        return orig_id, [{\"error\": \"ID not found in source file\"}]\n",
    "\n",
    "    image_info_list = raw_sample.get(\"image_info\", [])\n",
    "    if not image_info_list:\n",
    "        return orig_id, []\n",
    "\n",
    "    image_tasks = []\n",
    "    for img_info in image_info_list:\n",
    "        image_tasks.append(process_single_image_vlm(img_info, semaphore))\n",
    "    \n",
    "    enhanced_results = await asyncio.gather(*image_tasks)\n",
    "    \n",
    "    structured_captions = []\n",
    "    for i, res in enumerate(enhanced_results):\n",
    "        clean_res = str(res).replace(\"[Enhanced Captions]:\", \"\").strip()\n",
    "        \n",
    "        entry = {\n",
    "            \"image_index\": i + 1,\n",
    "            \"description\": clean_res\n",
    "        }\n",
    "        structured_captions.append(entry)\n",
    "        \n",
    "    return orig_id, structured_captions\n",
    "\n",
    "async def main_vlm_enhancement():\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"[Error] Input file not found: {INPUT_FILE}\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(SOURCE_DATA_FILE):\n",
    "        print(f\"[Error] Source data file not found: {SOURCE_DATA_FILE}\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Loading filtered data: {INPUT_FILE} ...\")\n",
    "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "        step1_data = json.load(f)\n",
    "    \n",
    "    print(f\"Loading source data (images): {SOURCE_DATA_FILE} ...\")\n",
    "    with open(SOURCE_DATA_FILE, 'r', encoding='utf-8') as f:\n",
    "        raw_source_list = json.load(f)\n",
    "\n",
    "    source_map = {}\n",
    "    for item in raw_source_list:\n",
    "        oid = item.get(\"original_sample_index\")\n",
    "        if oid is not None:\n",
    "            source_map[oid] = item\n",
    "            source_map[str(oid)] = item\n",
    "\n",
    "    print(f\"Source map built. Total items: {len(raw_source_list)}\")\n",
    "\n",
    "    if not step1_data:\n",
    "        print(\"[Warning] Input data is empty\")\n",
    "        return\n",
    "\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_VLM_TASKS)\n",
    "    tasks = []\n",
    "    \n",
    "    print(f\"Starting VLM processing for {len(step1_data)} items...\")\n",
    "    \n",
    "    for item in step1_data:\n",
    "        tasks.append(process_sample_by_id(item, source_map, semaphore))\n",
    "\n",
    "    results = await tqdm_asyncio.gather(*tasks)\n",
    "    \n",
    "    results_map = {oid: res for oid, res in results if oid is not None}\n",
    "\n",
    "    success_count = 0\n",
    "    for item in step1_data:\n",
    "        oid = item.get(\"original_sample_index\")\n",
    "        new_captions = results_map.get(oid, [])\n",
    "        item[\"model-enhanced captions\"] = new_captions\n",
    "        \n",
    "        if new_captions and isinstance(new_captions, list):\n",
    "            if len(new_captions) > 0:\n",
    "                first_desc = new_captions[0].get(\"description\", \"\")\n",
    "                if \"ERROR\" not in first_desc:\n",
    "                    success_count += 1\n",
    "            else:\n",
    "                success_count += 1\n",
    "\n",
    "    try:\n",
    "        print(f\"Saving results to {OUTPUT_FILE} ...\")\n",
    "        with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(step1_data, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"\\n[SUCCESS] Saved to: {OUTPUT_FILE}\")\n",
    "        print(f\"Success count: {success_count}/{len(step1_data)}\")\n",
    "        \n",
    "        if len(step1_data) > 0:\n",
    "             print(\"\\n[Preview Sample 0 - model-enhanced captions]:\")\n",
    "             print(json.dumps(step1_data[0].get(\"model-enhanced captions\"), indent=2, ensure_ascii=False))\n",
    "             \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save file: {e}\")\n",
    "\n",
    "await main_vlm_enhancement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa19e583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "INPUT_FILES_MAP = {\n",
    "    \"fleming\": \"step1_model_enhanced_captions_fleming.json\",\n",
    "    \"hulu\":    \"step1_model_enhanced_captions_hulu.json\",\n",
    "    \"lingshu\": \"step1_model_enhanced_captions_lingshu.json\",\n",
    "    \"qwenvl\":  \"step2_model_enhanced_captions_qwenvl.json\"\n",
    "}\n",
    "\n",
    "FILTERED_DATA_FILE = \"step2_distilled_output.json\"\n",
    "OUTPUT_FILE_STEP3_5 = \"step2_5_Integration_vl_output.json\"\n",
    "\n",
    "MAX_CONCURRENT_CONSENSUS = 20\n",
    "\n",
    "def build_lookup_map(file_path, model_name):\n",
    "    lookup = {}\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"[Warning] Model file not found: {file_path} ({model_name})\")\n",
    "        return lookup\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data_list = json.load(f)\n",
    "        for sample in data_list:\n",
    "            s_idx = sample.get(\"original_sample_index\") \n",
    "            if s_idx is None: continue\n",
    "            if s_idx not in lookup:\n",
    "                lookup[s_idx] = {}\n",
    "            img_descs = sample.get(\"model-enhanced captions\", [])\n",
    "            for img_item in img_descs:\n",
    "                i_idx = img_item.get(\"image_index\")\n",
    "                desc = img_item.get(\"description\", \"\")\n",
    "                if i_idx is not None:\n",
    "                    lookup[s_idx][i_idx] = desc\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to parse {model_name}: {e}\")\n",
    "    return lookup\n",
    "\n",
    "async def generate_consensus_async(s_idx, i_idx, desc_map, client, model):\n",
    "    valid_count = sum(1 for d in desc_map.values() if d and len(d) > 5)\n",
    "    if valid_count < 1:\n",
    "        for d in desc_map.values():\n",
    "            if d and len(d) > 5: return d \n",
    "        return \"ERROR: Insufficient model outputs for consensus.\"\n",
    "    prompt = CONSENSUS_PROMPT_TEMPLATE.format(\n",
    "        desc_fleming=desc_map.get(\"fleming\", \"N/A\"),\n",
    "        desc_hulu=desc_map.get(\"hulu\", \"N/A\"),\n",
    "        desc_lingshu=desc_map.get(\"lingshu\", \"N/A\"),\n",
    "        desc_qwenvl=desc_map.get(\"qwenvl\", \"N/A\")\n",
    "    )\n",
    "    try:\n",
    "        response = await get_response_async([], prompt, model, client)\n",
    "        return response['content'].strip()\n",
    "    except Exception as e:\n",
    "        return f\"ERROR_CONSENSUS_GEN: {str(e)}\"\n",
    "\n",
    "async def process_sample_step3_5(raw_sample, lookup_maps, semaphore, client, model):\n",
    "    s_idx = raw_sample.get(\"original_sample_index\")\n",
    "    if s_idx is None: return None\n",
    "\n",
    "    # [CRITICAL CHANGE] Directly inherit EVERYTHING from the distilled input file\n",
    "    output_entry = raw_sample.copy()\n",
    "    \n",
    "    # Initialize the new field\n",
    "    output_entry[\"consensus_image_descriptions\"] = [] \n",
    "\n",
    "    image_info_list = raw_sample.get(\"image_captions\") or raw_sample.get(\"original_captions\", [])\n",
    "    \n",
    "    if not image_info_list: \n",
    "        return output_entry\n",
    "\n",
    "    async with semaphore:\n",
    "        for i, img_info in enumerate(image_info_list):\n",
    "            image_idx = img_info.get(\"image_index\", i + 1)\n",
    "            \n",
    "            current_desc_map = {}\n",
    "            for model_name, map_data in lookup_maps.items():\n",
    "                val = map_data.get(s_idx, {}).get(image_idx, \"\")\n",
    "                current_desc_map[model_name] = val\n",
    "            \n",
    "            consensus_text = await generate_consensus_async(s_idx, image_idx, current_desc_map, client, model)\n",
    "            \n",
    "            img_result = {\n",
    "                \"image_index\": image_idx,\n",
    "                \"description\": consensus_text,\n",
    "            }\n",
    "            output_entry[\"consensus_image_descriptions\"].append(img_result)\n",
    "\n",
    "    return output_entry\n",
    "\n",
    "async def main_step3_5_ensemble_structure():\n",
    "    print(\"--- Step 3.5: Starting Multi-Model Integration ---\")\n",
    "    \n",
    "    if not os.path.exists(FILTERED_DATA_FILE):\n",
    "        print(f\"[Error] Input distilled file not found: {FILTERED_DATA_FILE}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading distilled data (Base): {FILTERED_DATA_FILE} ...\")\n",
    "    with open(FILTERED_DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        valid_samples_list = json.load(f)\n",
    "    print(f\"Base samples count: {len(valid_samples_list)}\")\n",
    "\n",
    "    print(\"Loading outputs from 4 models (for captions only)...\")\n",
    "    lookup_maps = {} \n",
    "    for name, path in INPUT_FILES_MAP.items():\n",
    "        lookup_maps[name] = build_lookup_map(path, name)\n",
    "    print(\"Indexes built.\")\n",
    "\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_CONSENSUS)\n",
    "    tasks = []\n",
    "\n",
    "    print(f\"Preparing to process {len(valid_samples_list)} samples...\")\n",
    "    \n",
    "    for valid_sample in valid_samples_list:\n",
    "        tasks.append(process_sample_step3_5(\n",
    "            valid_sample, \n",
    "            lookup_maps, \n",
    "            semaphore, \n",
    "            local_text_client, \n",
    "            local_text_model\n",
    "        ))\n",
    "\n",
    "    results = await tqdm_asyncio.gather(*tasks, desc=\"Integrating models\")\n",
    "    \n",
    "    results = [r for r in results if r is not None]\n",
    "    results.sort(key=lambda x: x.get(\"original_sample_index\", 0))\n",
    "\n",
    "    print(f\"Saving final output to: {OUTPUT_FILE_STEP3_5} ...\")\n",
    "    with open(OUTPUT_FILE_STEP3_5, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "    print(f\"\\n[SUCCESS] Integration complete.\")\n",
    "    print(f\"Output file: {OUTPUT_FILE_STEP3_5}\")\n",
    "    print(f\"Final output count: {len(results)}\")\n",
    "    \n",
    "    if len(results) > 0:\n",
    "        print(\"\\n[Preview Sample 0 Keys (Verification of Inheritance)]:\")\n",
    "        print(list(results[0].keys()))\n",
    "\n",
    "await main_step3_5_ensemble_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daefb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "INPUT_FILE = \"step2_5_Integration_vl_output.json\"\n",
    "OUTPUT_FILE = \"step3_Context_Enhanced_Caption.json\"\n",
    "MAX_CONCURRENT_TASKS = 2\n",
    "\n",
    "async def process_single_sample_generation(item, client, model):\n",
    "    oid = item.get(\"original_sample_index\")\n",
    "    \n",
    "    distilled_bg = item.get(\"distilled_background\", \"\")\n",
    "    keywords = item.get(\"extracted_keywords\", \"\")\n",
    "    context = item.get(\"context\", \"\")\n",
    "    \n",
    "    original_captions = item.get(\"consensus_image_descriptions\", [])\n",
    "    if not original_captions:\n",
    "        return oid, {\"status\": \"skipped\", \"message\": \"No images\"}\n",
    "\n",
    "    if \"ERROR\" in str(distilled_bg) or \"SKIPPED\" in str(distilled_bg):\n",
    "        distilled_bg = \"Not available.\"\n",
    "    \n",
    "    captions_json_str = json.dumps(original_captions, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    prompt = ENHANCED_CAPTION_PROMPT_TEMPLATE.format(\n",
    "        distilled_background=distilled_bg,\n",
    "        keywords=keywords,\n",
    "        context=context,\n",
    "        vl_captions_json=captions_json_str\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = await get_response_async(\n",
    "            prev_messages=[], \n",
    "            next_content=prompt, \n",
    "            model=model, \n",
    "            client=client\n",
    "        )\n",
    "        content = response['content'].strip()\n",
    "        \n",
    "        cleaned_content = content\n",
    "        if \"```\" in content:\n",
    "            match = re.search(r\"```(?:json)?(.*?)```\", content, re.DOTALL)\n",
    "            if match:\n",
    "                cleaned_content = match.group(1).strip()\n",
    "        \n",
    "        parsed_json = {}\n",
    "        try:\n",
    "            parsed_json = json.loads(cleaned_content)\n",
    "        except json.JSONDecodeError:\n",
    "            return oid, {\n",
    "                \"status\": \"partial_error\", \n",
    "                \"raw_output\": content, \n",
    "                \"message\": \"Failed to parse JSON output\"\n",
    "            }\n",
    "\n",
    "        root_key = \"Context_Enhanced_Captions\"\n",
    "        standardized_result = {}\n",
    "\n",
    "        if root_key in parsed_json:\n",
    "            core_data = parsed_json[root_key]\n",
    "            if \"observations\" in core_data: \n",
    "                standardized_result = core_data\n",
    "            else:\n",
    "                return oid, {\n",
    "                    \"status\": \"partial_error\", \n",
    "                    \"raw_output\": content, \n",
    "                    \"message\": \"Parsed JSON missing 'observations' or 'interpretations' fields.\"\n",
    "                }\n",
    "        else:\n",
    "            if \"observations\" in parsed_json: \n",
    "                standardized_result = parsed_json\n",
    "            else:\n",
    "                return oid, {\n",
    "                    \"status\": \"partial_error\", \n",
    "                    \"raw_output\": content, \n",
    "                    \"message\": f\"Root key '{root_key}' not found in output.\"\n",
    "                }\n",
    "\n",
    "        return oid, {\n",
    "            \"status\": \"success\", \n",
    "            \"context_enhanced_data\": standardized_result, \n",
    "            \"raw_output_str\": cleaned_content\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return oid, {\"status\": \"error\", \"error_message\": str(e)}\n",
    "\n",
    "\n",
    "async def main_generation_from_file(input_path, output_path, client, model):\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"[Error] Input file not found: {input_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Reading input file: {input_path} ...\")\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        current_data = json.load(f)\n",
    "\n",
    "    if not current_data:\n",
    "        print(\"Data is empty.\")\n",
    "        return\n",
    "\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_TASKS)\n",
    "\n",
    "    async def sem_task(item):\n",
    "        async with semaphore:\n",
    "            return await process_single_sample_generation(item, client, model)\n",
    "\n",
    "    tasks = [sem_task(item) for item in current_data]\n",
    "\n",
    "    print(f\"Starting generation (Samples: {len(tasks)})...\")\n",
    "    results = await tqdm_asyncio.gather(*tasks)\n",
    "\n",
    "    results_map = {oid: res for oid, res in results if oid is not None}\n",
    "    \n",
    "    success_count = 0\n",
    "    for item in current_data:\n",
    "        oid = item.get(\"original_sample_index\")\n",
    "        \n",
    "        if oid in results_map:\n",
    "            res = results_map[oid]\n",
    "            \n",
    "            if res['status'] == 'success':\n",
    "                data = res['context_enhanced_data']\n",
    "                obs_dict = data.get(\"observations\", {})\n",
    "                \n",
    "                \n",
    "                structured_captions = []\n",
    "                original_caps = item.get(\"image_captions\", [])\n",
    "                \n",
    "                for i in range(len(original_caps)):\n",
    "                    idx = i + 1\n",
    "                    key = f\"Image {idx}\"\n",
    "                    \n",
    "                    entry = {\n",
    "                        \"image_index\": idx,\n",
    "                        \"observation\": obs_dict.get(key, \"Not found\")\n",
    "                    }\n",
    "                    structured_captions.append(entry)\n",
    "                \n",
    "                item[\"context_enhanced_captions\"] = structured_captions\n",
    "                \n",
    "                obs_summary = obs_dict.get(\"summary\") or obs_dict.get(\"Summary\") or \"\"\n",
    "                \n",
    "                item[\"context_enhanced_summary\"] = {\n",
    "                    \"observation_summary\": obs_summary\n",
    "                }\n",
    "                \n",
    "                success_count += 1\n",
    "                \n",
    "            elif res['status'] == 'partial_error':\n",
    "                item[\"_error_info\"] = res.get(\"message\")\n",
    "                item[\"_raw_error_output\"] = res.get(\"raw_output\", \"\")\n",
    "            else:\n",
    "                item[\"_error_info\"] = \"ERROR_GENERATION\"\n",
    "        else:\n",
    "             item[\"_error_info\"] = \"MISSING\"\n",
    "\n",
    "    try:\n",
    "        print(f\"Saving output file: {output_path} ...\")\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(current_data, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"Output file saved successfully!\")\n",
    "\n",
    "        print(f\"\\n[SUCCESS] All tasks completed! Success: {success_count}/{len(current_data)}\")\n",
    "        \n",
    "        if current_data:\n",
    "            print(\"\\n[Preview Sample 0 Keys]:\")\n",
    "            print(list(current_data[0].keys()))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save file: {e}\")\n",
    "\n",
    "await main_generation_from_file(\n",
    "    INPUT_FILE, \n",
    "    OUTPUT_FILE, \n",
    "    local_text_client, \n",
    "    local_text_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "SOURCE_FILE = \"qa_generation_quickly.json\"\n",
    "TARGET_FILE = \"step3_Context_Enhanced_Caption.json\"\n",
    "OUTPUT_FILE = \"step3_Context_Enhanced_Caption.json\"\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(SOURCE_FILE) or not os.path.exists(TARGET_FILE):\n",
    "        print(f\"Error: One or both input files not found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading source data from {SOURCE_FILE}...\")\n",
    "    with open(SOURCE_FILE, 'r', encoding='utf-8') as f:\n",
    "        source_data = json.load(f)\n",
    "\n",
    "    print(f\"Loading target data from {TARGET_FILE}...\")\n",
    "    with open(TARGET_FILE, 'r', encoding='utf-8') as f:\n",
    "        target_data = json.load(f)\n",
    "\n",
    "    source_lookup = {}\n",
    "    for item in source_data:\n",
    "        oid = item.get(\"original_sample_index\")\n",
    "        if oid is not None:\n",
    "            images_map = {}\n",
    "            for img in item.get(\"image_info\", []):\n",
    "                idx = img.get(\"index\")\n",
    "                if idx is not None:\n",
    "                    images_map[idx] = {\n",
    "                        \"fig_id\": img.get(\"fig_id\", \"\"),\n",
    "                        \"subfig_label\": img.get(\"subfig_label\", \"\")\n",
    "                    }\n",
    "            source_lookup[oid] = images_map\n",
    "\n",
    "    print(\"Merging data fields...\")\n",
    "    updated_samples_count = 0\n",
    "    updated_images_count = 0\n",
    "\n",
    "    for item in target_data:\n",
    "        oid = item.get(\"original_sample_index\")\n",
    "        \n",
    "        if oid in source_lookup:\n",
    "            current_image_map = source_lookup[oid]\n",
    "            captions_list = item.get(\"context_enhanced_captions\", [])\n",
    "            \n",
    "            sample_updated = False\n",
    "            if isinstance(captions_list, list):\n",
    "                for caption_obj in captions_list:\n",
    "                    img_idx = caption_obj.get(\"image_index\")\n",
    "                    \n",
    "                    if img_idx in current_image_map:\n",
    "                        meta_data = current_image_map[img_idx]\n",
    "                        caption_obj[\"fig_id\"] = meta_data[\"fig_id\"]\n",
    "                        caption_obj[\"subfig_label\"] = meta_data[\"subfig_label\"]\n",
    "                        updated_images_count += 1\n",
    "                        sample_updated = True\n",
    "            \n",
    "            if sample_updated:\n",
    "                updated_samples_count += 1\n",
    "\n",
    "    print(f\"Process complete.\")\n",
    "    print(f\"Total samples matched and updated: {updated_samples_count}\")\n",
    "    print(f\"Total individual image entries updated: {updated_images_count}\")\n",
    "\n",
    "    print(f\"Saving to {OUTPUT_FILE}...\")\n",
    "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(target_data, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(\"Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d202ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Step 7: Generate Visual Recognition Simple QA (Structured Data Adapted Version) ====================\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "# 1. Helper function: Extract and concatenate Observation directly from structured data\n",
    "def extract_observation_text(item):\n",
    "    \"\"\"\n",
    "    Extract all observation text from structured context_enhanced_captions\n",
    "    and concatenate into a format suitable for Prompt input.\n",
    "    \"\"\"\n",
    "    captions_list = item.get(\"context_enhanced_captions\", [])\n",
    "    \n",
    "    # Get Observation Summary (if available)\n",
    "    summary_data = item.get(\"context_enhanced_summary\", {})\n",
    "    obs_summary = summary_data.get(\"observation_summary\", \"\")\n",
    "    \n",
    "    # Concatenate Observation for each Image\n",
    "    combined_obs_parts = []\n",
    "    for entry in captions_list:\n",
    "        idx = entry.get(\"image_index\")\n",
    "        obs_text = entry.get(\"observation\", \"\")\n",
    "        if obs_text and obs_text != \"Not found\":\n",
    "            combined_obs_parts.append(f\"[Image {idx}]: {obs_text}\")\n",
    "    \n",
    "    # Add Summary\n",
    "    if obs_summary:\n",
    "        combined_obs_parts.append(f\"[Summary]: {obs_summary}\")\n",
    "        \n",
    "    if not combined_obs_parts:\n",
    "        return None\n",
    "        \n",
    "    return \"\\n\".join(combined_obs_parts)\n",
    "\n",
    "# 2. Generation function\n",
    "async def generate_visual_element_qa_async(obs_str):\n",
    "    prompt = VISUAL_ELEMENT_QA_PROMPT_TEMPLATE.format(\n",
    "        Observation=obs_str\n",
    "    )\n",
    "    try:\n",
    "        result = await get_response_async([], prompt, local_text_model, local_text_client)\n",
    "        \n",
    "        # Parse JSON (handle Markdown wrapping)\n",
    "        content = result[\"content\"].strip()\n",
    "        if \"```\" in content:\n",
    "            import re\n",
    "            match = re.search(r\"```(?:json)?(.*?)```\", content, re.DOTALL)\n",
    "            if match:\n",
    "                content = match.group(1).strip()\n",
    "        \n",
    "        return json.loads(content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating Visual QA: {e}\")\n",
    "        return []\n",
    "\n",
    "# 3. Single task logic (adapted to new structure)\n",
    "async def run_visual_step_task(step1_data):\n",
    "    idx = step1_data.get(\"original_sample_index\")\n",
    "    context = step1_data.get(\"context\", \"\")\n",
    "    \n",
    "    # [Modification] No longer parse string, call extraction function instead\n",
    "    obs_str = extract_observation_text(step1_data)\n",
    "    \n",
    "    if not obs_str:\n",
    "        return None, {\"original_sample_index\": idx, \"error\": \"No observations found in structured data\"}\n",
    "\n",
    "    try:\n",
    "        qa_list = await generate_visual_element_qa_async(obs_str)\n",
    "    except Exception as e:\n",
    "        return None, {\"original_sample_index\": idx, \"error\": f\"Gen failed: {e}\"}\n",
    "\n",
    "    if not qa_list:\n",
    "        return None, {\"original_sample_index\": idx, \"error\": \"No QA returned from LLM\"}\n",
    "\n",
    "    result_data = {\n",
    "        \"original_sample_index\": int(idx) if idx is not None else -1,\n",
    "        \"visual_qa\": qa_list[0], \n",
    "        \"input_observation\": obs_str,\n",
    "        \"extracted_keywords\": step1_data.get(\"extracted_keywords\", \"\"),\n",
    "        \"distilled_background\": step1_data.get(\"distilled_background\", \"\"),\n",
    "        \"context\": context,\n",
    "        \"context_enhanced_captions\": step1_data.get(\"context_enhanced_captions\"),\n",
    "        \"context_enhanced_summary\": step1_data.get(\"context_enhanced_summary\") \n",
    "    }\n",
    "    \n",
    "    return result_data, None\n",
    "\n",
    "# 4. Main execution logic\n",
    "async def main_visual_element_gen():\n",
    "    INPUT_FILE = \"step3_Context_Enhanced_Caption.json\"\n",
    "    OUTPUT_FILE = \"step4_visual-element_qa_output.json\"\n",
    "    FAILED_FILE = \"step4_visual-element_qa_output_failed.json\"\n",
    "    \n",
    "    print(f\"[INFO] Loading data from {INPUT_FILE}...\")\n",
    "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        data_list = json.load(f)\n",
    "        \n",
    "    # Run only the first 5 items for testing (if needed)\n",
    "    # data_list = data_list[:5]\n",
    "\n",
    "    CONCURRENT_LIMIT = 10\n",
    "    semaphore = asyncio.Semaphore(CONCURRENT_LIMIT)\n",
    "\n",
    "    async def wrapped(item):\n",
    "        async with semaphore:\n",
    "            return await run_visual_step_task(item)\n",
    "\n",
    "    tasks = [wrapped(item) for item in data_list]\n",
    "    \n",
    "    print(f\"[INFO] Starting Visual QA Gen for {len(tasks)} samples...\")\n",
    "    results = await tqdm_asyncio.gather(*tasks)\n",
    "\n",
    "    success_results = []\n",
    "    failed_results = []\n",
    "\n",
    "    for passed, failed in results:\n",
    "        if passed:\n",
    "            success_results.append(passed)\n",
    "        else:\n",
    "            failed_results.append(failed)\n",
    "\n",
    "    print(f\"[INFO] Saving {len(success_results)} VALID items to {OUTPUT_FILE}...\")\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(success_results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    if failed_results:\n",
    "        print(f\"[INFO] Saving {len(failed_results)} FAILED items to {FAILED_FILE}...\")\n",
    "        with open(FAILED_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(failed_results, f, indent=4, ensure_ascii=False)\n",
    "            \n",
    "    print(f\"\\n[INFO] Task Complete. Success Rate: {len(success_results)}/{len(data_list)}\")\n",
    "\n",
    "# Start\n",
    "if 'VISUAL_ELEMENT_QA_PROMPT_TEMPLATE' not in globals():\n",
    "    print(\"[WARNING] Please define the Prompt Template first!\")\n",
    "else:\n",
    "    await main_visual_element_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de762b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "\n",
    "INCLUDE_FULL_QA = False\n",
    "def extract_visual_info(visual_qa_data, include_full=False):\n",
    "    \"\"\"\n",
    "    Function: Filter data based on the switch and return a standard JSON string.\n",
    "    \"\"\"\n",
    "    if not visual_qa_data:\n",
    "        return \"{}\"\n",
    "\n",
    "    # 1. Create an empty dictionary to store filtered data\n",
    "    filtered_data = {}\n",
    "\n",
    "    # Get raw QA pairs\n",
    "    raw_pairs = visual_qa_data.get(\"qa_pairs\", {})\n",
    "    processed_pairs = {}\n",
    "\n",
    "    # 2. Filter QA content\n",
    "    for i, (original_key, val) in enumerate(raw_pairs.items(), start=1):\n",
    "        if include_full:\n",
    "            # True: Keep full Q and A\n",
    "            # Keep original Key (e.g. \"qa_1\") or use \"visual_qa_1\"\n",
    "            processed_pairs[original_key] = {\n",
    "                \"question\": val.get(\"question\", \"\"),\n",
    "                \"answer\": val.get(\"answer\", \"\")\n",
    "            }\n",
    "        else:\n",
    "            # False: Only extract Answer\n",
    "            # 2. Generate new names \"visual_fact_1\", \"visual_fact_2\"...\n",
    "            new_key_name = f\"{i}\"\n",
    "            processed_pairs[new_key_name] = val.get(\"answer\", \"\")\n",
    "\n",
    "    # Put processed QA pairs into result dictionary\n",
    "    filtered_data[\"visual_facts\"] = processed_pairs\n",
    "\n",
    "\n",
    "    # 3. Finally convert to JSON string and return\n",
    "    return json.dumps(filtered_data, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "def split_caption_data(item):\n",
    "    captions_list = item.get(\"context_enhanced_captions\", [])\n",
    "    summary_data = item.get(\"context_enhanced_summary\", {})\n",
    "    obs_parts = []\n",
    "    for entry in captions_list:\n",
    "        idx = entry.get(\"image_index\")\n",
    "        f_id = entry.get(\"fig_id\", \"\")\n",
    "        s_label = entry.get(\"subfig_label\", \"\")\n",
    "        id_prefix = f\":{f_id} {s_label} :\" if (f_id or s_label) else \"\"\n",
    "\n",
    "        obs = entry.get(\"observation\", \"\")\n",
    "        if obs and obs != \"Not found\":\n",
    "            obs_parts.append(f\"{id_prefix} [Image {idx}]: {obs}\")\n",
    "\n",
    "    if summary_data:\n",
    "        if summary_data.get(\"observation_summary\"):\n",
    "            obs_parts.append(\n",
    "                f\"[Observation Summary]: {summary_data['observation_summary']}\"\n",
    "            )\n",
    "\n",
    "    return \"\\n\".join(obs_parts), \"\"\n",
    "\n",
    "\n",
    "\n",
    "# Main function\n",
    "async def generate_logic_chain_async(visual_fact, obs_str, int_str, context_str):\n",
    "\n",
    "    prompt = LOGIC_CHAIN_PROMPT_TEMPLATE.format(\n",
    "        observation=obs_str,\n",
    "        context=context_str\n",
    "    )\n",
    "\n",
    "    retry_count = 3\n",
    "\n",
    "    for attempt in range(retry_count):\n",
    "        try:\n",
    "            # Reuse get_response_async\n",
    "            # Note: Text input only here, so pack_content uses None for image\n",
    "            content = openai_pack_content(prompt, None)\n",
    "            result = await get_response_async([], content, local_text_model, local_text_client)\n",
    "\n",
    "\n",
    "            parsed_json = process_qa_output(result[\"content\"]) # Reuse previous process_qa_output\n",
    "            return parsed_json\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating Logic Chain: {e}\")\n",
    "            if attempt < retry_count - 1:\n",
    "                await asyncio.sleep(1)  # Wait and retry\n",
    "    return None\n",
    "\n",
    "async def run_logic_chain_task(item):\n",
    "    # 1. Prepare input data\n",
    "    idx = item.get(\"original_sample_index\")\n",
    "    visual_qa_data = item.get(\"visual_qa\", {})\n",
    "    context_str = item.get(\"context\", \"\")\n",
    "\n",
    "    # Extract data from visual_qa\n",
    "    visual_fact = extract_visual_info(visual_qa_data, include_full=INCLUDE_FULL_QA)\n",
    "\n",
    "    # Extract Observation and Interpretation\n",
    "    obs_str, int_str = split_caption_data(item)\n",
    "\n",
    "    if not visual_fact or not context_str:\n",
    "        return None, {\"original_sample_index\": idx, \"error\": \"Missing visual_qa or context\"}\n",
    "    int_str = None\n",
    "    # 2. Call LLM to generate\n",
    "    retry_count = 3\n",
    "    for attempt in range(retry_count):\n",
    "        try:\n",
    "            logic_chain_json = await generate_logic_chain_async(visual_fact, obs_str, int_str, context_str)\n",
    "            if logic_chain_json is None:\n",
    "                raise Exception(\"Logic chain generation returned None\")\n",
    "            break  # Break loop on success\n",
    "        except Exception as e:\n",
    "            if attempt >= retry_count - 1:\n",
    "                return None, {\"original_sample_index\": idx, \"error\": f\"Gen failed: {e}\"}\n",
    "            else:\n",
    "                await asyncio.sleep(2)  # Wait and retry\n",
    "\n",
    "\n",
    "\n",
    "    visual_fact = extract_visual_info(visual_qa_data, include_full=INCLUDE_FULL_QA)\n",
    "    # 3. Assemble results\n",
    "    result_data = item.copy() # Copy all fields from Step 5\n",
    "\n",
    "    # New fields\n",
    "    result_data[\"logic_chain\"] = logic_chain_json\n",
    "\n",
    "    result_data[\"debug_model_input\"] = {\n",
    "        \"observation_input\": obs_str,\n",
    "        \"interpretation_input\": int_str\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        result_data[\"debug_input_visual_facts\"] = json.loads(visual_fact)\n",
    "    except:\n",
    "        result_data[\"debug_input_visual_facts\"] = visual_fact\n",
    "\n",
    "    return result_data, None\n",
    "\n",
    "\n",
    "\n",
    "async def main_logic_chain_gen():\n",
    "\n",
    "    INPUT_FILE = \"step4_visual-element_qa_output.json\"\n",
    "    OUTPUT_FILE = \"step5_logic_chain_output.json\"\n",
    "    FAILED_FILE = \"step5_logic_chain_failed.json\"\n",
    "\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"[ERROR] Input file {INPUT_FILE} not found!\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] Loading data from {INPUT_FILE}...\")\n",
    "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        data_list = json.load(f)\n",
    "\n",
    "    CONCURRENT_LIMIT = 32\n",
    "    semaphore = asyncio.Semaphore(CONCURRENT_LIMIT)\n",
    "\n",
    "    async def wrapped(item):\n",
    "        async with semaphore:\n",
    "            return await run_logic_chain_task(item)\n",
    "\n",
    "    tasks = [wrapped(item) for item in data_list]\n",
    "\n",
    "    print(f\"[INFO] Starting Logic Chain Generation for {len(tasks)} samples...\")\n",
    "    results = await tqdm_asyncio.gather(*tasks)\n",
    "\n",
    "    success_results = []\n",
    "    failed_results = []\n",
    "\n",
    "    for passed, failed in results:\n",
    "        if passed:\n",
    "            success_results.append(passed)\n",
    "        else:\n",
    "            failed_results.append(failed)\n",
    "\n",
    "    print(f\"[INFO] Saving {len(success_results)} VALID items to {OUTPUT_FILE}...\")\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(success_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    if failed_results:\n",
    "        print(f\"[INFO] Saving {len(failed_results)} FAILED items to {FAILED_FILE}...\")\n",
    "        with open(FAILED_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(failed_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\n[INFO] Logic Chain Task Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8902dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'LOGIC_CHAIN_PROMPT_TEMPLATE' not in globals():\n",
    "    print(\"[ERROR] Please define LOGIC_CHAIN_PROMPT_TEMPLATE first!\")\n",
    "else:\n",
    "    await main_logic_chain_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c1c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.asyncio import tqdm_asyncio\n",
    "async def generate_logic_based_qa_async(logic_chain, visual_evidence, original_text):\n",
    "    prompt = OPEN_ENDED_QA_GENERATION_PROMPT_TEMPLATE.format(\n",
    "        logic_chain=logic_chain,\n",
    "        visual_evidence=visual_evidence,\n",
    "        original_text=original_text\n",
    "    )\n",
    "    \n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            content = openai_pack_content(prompt, None)  \n",
    "            result = await get_response_async([], content, local_text_model, local_text_client)\n",
    "\n",
    "            full_response = result[\"content\"].strip()\n",
    "\n",
    "            # process response\n",
    "            result = process_qa_output(full_response)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"--- [Error] (Attempt {attempt + 1}/{max_retries}): {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                print(\"--- Max retries reached for logic-based QA generation.\")\n",
    "                raise e\n",
    "            await asyncio.sleep(5)\n",
    "\n",
    "async def run_logic_based_qa_task(item):\n",
    "    idx = item.get(\"original_sample_index\")\n",
    "    logic_chain = item.get(\"logic_chain\", {})\n",
    "    visual_qa_data = item.get(\"visual_qa\", {})\n",
    "    context_str = item.get(\"context\", \"\")\n",
    "\n",
    "    obs_str, int_str = split_caption_data(item)\n",
    "\n",
    "    try:\n",
    "        qa_pair = await generate_logic_based_qa_async(\n",
    "            json.dumps(logic_chain[0], ensure_ascii=False, indent=2),\n",
    "            obs_str,\n",
    "            context_str\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        return None, {\"original_sample_index\": idx, \"error\": f\"Gen failed: {e}\"}\n",
    "    if not qa_pair:\n",
    "        return None, {\"original_sample_index\": idx, \"error\": \"No QA returned from LLM\"}\n",
    "    \n",
    "    result_data = {\n",
    "        \"original_sample_index\": int(idx) if idx is not None else -1,\n",
    "        \"basic_qa\": qa_pair,\n",
    "        \"input_observation\": obs_str,\n",
    "        \"input_context\": context_str,\n",
    "        \"input_logic_chain\": logic_chain\n",
    "    }\n",
    "\n",
    "    return result_data, None\n",
    "\n",
    "async def main_logic_based_qa_gen():\n",
    "    INPUT_FILE = \"step5_logic_chain_output.json\"\n",
    "    OUTPUT_FILE = \"step6_logic_based_qa_output.json\"\n",
    "    FAILED_FILE = \"step6_logic_based_qa_failed.json\"\n",
    "    \n",
    "    print(f\"[INFO] Loading data from {INPUT_FILE}...\")\n",
    "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        data_list = json.load(f)\n",
    "        \n",
    "    CONCURRENT_LIMIT = 10\n",
    "    semaphore = asyncio.Semaphore(CONCURRENT_LIMIT)\n",
    "    tasks = []\n",
    "    for item in data_list:\n",
    "        tasks.append(run_logic_based_qa_task(item))\n",
    "        \n",
    "    print(f\"[INFO] Starting Logic-Based QA Generation for {len(tasks)} samples...\")\n",
    "    results = await tqdm_asyncio.gather(*tasks)\n",
    "    success_results = []\n",
    "    failed_results = []\n",
    "    for passed, failed in results:\n",
    "        if passed:\n",
    "            success_results.append(passed)\n",
    "        else:\n",
    "            failed_results.append(failed)\n",
    "    print(f\"[INFO] Saving {len(success_results)} VALID items to {OUTPUT_FILE}...\")\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(success_results, f, indent=2, ensure_ascii=False)\n",
    "    if failed_results:\n",
    "        print(f\"[INFO] Saving {len(failed_results)} FAILED items to {FAILED_FILE}...\")\n",
    "        with open(FAILED_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(failed_results, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\n[INFO] Logic-Based QA Task Complete.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c74e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "await main_logic_based_qa_gen()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_qa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
